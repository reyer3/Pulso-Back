-- ./app/etl/utils.py --
"""
ðŸ› ï¸ ETL Utilities
Production utilities for testing, debugging, and maintenance

Features:
- Quick testing of individual components
- Data validation and quality checks
- Performance benchmarking
- Manual recovery tools
"""

import asyncio
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Optional
import json
import logging

from app.etl.config import ETLConfig, ExtractionMode
from app.etl.extractors.bigquery_extractor import BigQueryExtractor
from app.etl.loaders.postgres_loader import PostgresLoader
from app.etl.watermarks import get_watermark_manager
from app.etl.pipelines.extraction_pipeline import get_pipeline


class ETLTester:
    """Utility class for testing ETL components"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    async def test_bigquery_connection(self) -> Dict[str, Any]:
        """Test BigQuery connectivity and permissions"""
        try:
            extractor = BigQueryExtractor()
            
            # Test basic query
            test_result = await extractor.test_query("SELECT 1 as test, CURRENT_TIMESTAMP() as now")
            
            return {
                "status": "success",
                "connection": "OK",
                "permissions": "READ access confirmed",
                "test_query": test_result
            }
            
        except Exception as e:
            return {
                "status": "failed",
                "error": str(e),
                "suggestion": "Check Google Cloud credentials and BigQuery permissions"
            }
    
    async def test_postgres_connection(self) -> Dict[str, Any]:
        """Test PostgreSQL connectivity and permissions"""
        try:
            loader = PostgresLoader()
            
            # Test simple insert
            test_data = [{"test_id": 1, "test_name": "ETL Test", "test_timestamp": datetime.now()}]
            
            result = await loader.load_data_batch(
                table_name="etl_test_table",
                data=test_data,
                primary_key=["test_id"],
                upsert=True,
                create_table=True
            )
            
            return {
                "status": "success",
                "connection": "OK",
                "permissions": "READ/WRITE access confirmed",
                "test_load": result.__dict__
            }
            
        except Exception as e:
            return {
                "status": "failed",
                "error": str(e),
                "suggestion": "Check PostgreSQL connection string and permissions"
            }
    
    async def test_table_extraction(self, table_name: str) -> Dict[str, Any]:
        """Test extraction for a specific table"""
        try:
            extractor = BigQueryExtractor()
            
            # Test the configured query
            config = ETLConfig.get_config(table_name)
            base_query = ETLConfig.get_query(table_name)
            
            # Build query with small date range for testing
            test_date = datetime.now(timezone.utc) - timedelta(days=1)
            incremental_filter = f"{config.incremental_column} >= '{test_date.strftime('%Y-%m-%d')}'"
            test_query = base_query.replace("{incremental_filter}", incremental_filter)
            
            # Test query execution
            test_result = await extractor.test_query(test_query)
            
            return {
                "status": "success",
                "table_name": table_name,
                "query_test": test_result,
                "config": {
                    "primary_key": config.primary_key,
                    "incremental_column": config.incremental_column,
                    "batch_size": config.batch_size
                }
            }
            
        except Exception as e:
            return {
                "status": "failed",
                "table_name": table_name,
                "error": str(e)
            }
    
    async def benchmark_table_extraction(self, table_name: str, days_back: int = 1) -> Dict[str, Any]:
        """Benchmark extraction performance for a table"""
        try:
            pipeline = get_pipeline()
            
            start_time = time.time()
            
            # Force extraction for benchmark
            load_result = await pipeline.extract_and_load_table(
                table_name=table_name,
                mode=ExtractionMode.INCREMENTAL,
                force=True
            )
            
            total_time = time.time() - start_time
            
            # Calculate performance metrics
            records_per_second = load_result.total_records / max(total_time, 0.1)
            
            return {
                "status": "success",
                "table_name": table_name,
                "performance": {
                    "total_records": load_result.total_records,
                    "total_time_seconds": round(total_time, 2),
                    "records_per_second": round(records_per_second, 2),
                    "load_time_seconds": load_result.load_duration_seconds
                },
                "result": load_result.__dict__
            }
            
        except Exception as e:
            return {
                "status": "failed",
                "table_name": table_name,
                "error": str(e)
            }


class ETLValidator:
    """Data quality validation utilities"""
    
    async def validate_table_data(self, table_name: str) -> Dict[str, Any]:
        """Validate data quality for a table"""
        try:
            # Get table configuration
            config = ETLConfig.get_config(table_name)
            
            # Get watermark info
            watermark_manager = await get_watermark_manager()
            watermark = await watermark_manager.get_watermark(table_name)
            
            # Basic validations
            validations = {
                "table_configured": True,
                "has_watermark": watermark is not None,
                "recent_extraction": False,
                "expected_records": False,
                "primary_key_valid": len(config.primary_key) > 0
            }
            
            if watermark:
                # Check if extraction is recent
                hours_since_extraction = (datetime.now(timezone.utc) - watermark.last_extracted_at).total_seconds() / 3600
                validations["recent_extraction"] = hours_since_extraction < (config.refresh_frequency_hours * 2)
                
                # Check if we have expected minimum records
                validations["expected_records"] = watermark.records_extracted >= config.min_expected_records
            
            # Calculate overall health
            passed_validations = sum(1 for v in validations.values() if v)
            health_score = (passed_validations / len(validations)) * 100
            
            return {
                "table_name": table_name,
                "health_score": round(health_score, 2),
                "validations": validations,
                "last_extraction": {
                    "timestamp": watermark.last_extracted_at.isoformat() if watermark else None,
                    "status": watermark.last_extraction_status if watermark else "never",
                    "records": watermark.records_extracted if watermark else 0
                } if watermark else None,
                "recommendations": _get_health_recommendations(validations, config)
            }
            
        except Exception as e:
            return {
                "table_name": table_name,
                "error": str(e),
                "health_score": 0
            }


class ETLRecovery:
    """Recovery and maintenance utilities"""
    
    async def reset_table_watermark(self, table_name: str, reset_to_date: Optional[datetime] = None) -> Dict[str, Any]:
        """Reset watermark for a table (use with caution)"""
        try:
            watermark_manager = await get_watermark_manager()
            
            if reset_to_date is None:
                # Reset to 7 days ago by default
                reset_to_date = datetime.now(timezone.utc) - timedelta(days=7)
            
            await watermark_manager.reset_watermark(table_name, reset_to_date)
            
            return {
                "status": "success",
                "table_name": table_name,
                "reset_to": reset_to_date.isoformat(),
                "message": f"Watermark reset - next extraction will process data since {reset_to_date}"
            }
            
        except Exception as e:
            return {
                "status": "failed",
                "table_name": table_name,
                "error": str(e)
            }
    
    async def force_full_refresh(self, table_name: str) -> Dict[str, Any]:
        """Force a full refresh of a table"""
        try:
            pipeline = get_pipeline()
            
            load_result = await pipeline.extract_and_load_table(
                table_name=table_name,
                mode=ExtractionMode.FULL_REFRESH,
                force=True
            )
            
            return {
                "status": "success",
                "table_name": table_name,
                "mode": "full_refresh",
                "result": load_result.__dict__
            }
            
        except Exception as e:
            return {
                "status": "failed",
                "table_name": table_name,
                "error": str(e)
            }


def _get_health_recommendations(validations: Dict[str, bool], config) -> List[str]:
    """Get health recommendations based on validation results"""
    recommendations = []
    
    if not validations.get("has_watermark"):
        recommendations.append("Run initial extraction to establish watermark")
    
    if not validations.get("recent_extraction"):
        recommendations.append(f"Extraction overdue - should run every {config.refresh_frequency_hours} hours")
    
    if not validations.get("expected_records"):
        recommendations.append(f"Low record count - expected at least {config.min_expected_records} records")
    
    if not validations.get("primary_key_valid"):
        recommendations.append("Configure valid primary key for UPSERT operations")
    
    if not recommendations:
        recommendations.append("Table appears healthy - no immediate actions needed")
    
    return recommendations


# ðŸŽ¯ Convenience functions for easy CLI usage

async def quick_test() -> Dict[str, Any]:
    """Quick test of all ETL components"""
    tester = ETLTester()
    
    results = {
        "bigquery": await tester.test_bigquery_connection(),
        "postgres": await tester.test_postgres_connection(),
        "configured_tables": len(ETLConfig.list_tables()),
        "dashboard_tables": ETLConfig.get_dashboard_tables()
    }
    
    return results


async def validate_all_tables() -> Dict[str, Any]:
    """Validate data quality for all configured tables"""
    validator = ETLValidator()
    
    results = {}
    overall_health = []
    
    for table_name in ETLConfig.list_tables():
        result = await validator.validate_table_data(table_name)
        results[table_name] = result
        overall_health.append(result.get("health_score", 0))
    
    # Calculate system-wide health
    system_health = sum(overall_health) / len(overall_health) if overall_health else 0
    
    return {
        "system_health_score": round(system_health, 2),
        "tables": results,
        "summary": {
            "total_tables": len(results),
            "healthy_tables": len([r for r in results.values() if r.get("health_score", 0) >= 80]),
            "degraded_tables": len([r for r in results.values() if 50 <= r.get("health_score", 0) < 80]),
            "unhealthy_tables": len([r for r in results.values() if r.get("health_score", 0) < 50])
        }
    }


async def emergency_recovery() -> Dict[str, Any]:
    """Emergency recovery for all failed extractions"""
    recovery = ETLRecovery()
    pipeline = get_pipeline()
    
    # Get cleanup results
    cleanup_result = await pipeline.cleanup_and_recover()
    
    # Additional recovery if needed
    recovery_results = []
    
    watermark_manager = await get_watermark_manager()
    failed_extractions = await watermark_manager.get_failed_extractions()
    
    for failed in failed_extractions:
        # Try to recover each failed table
        result = await recovery.force_full_refresh(failed.table_name)
        recovery_results.append(result)
    
    return {
        "cleanup": cleanup_result,
        "recovery_attempts": recovery_results,
        "total_recovered": len([r for r in recovery_results if r.get("status") == "success"])
    }


# ðŸ”§ CLI-style functions for manual testing

if __name__ == "__main__":
    import asyncio
    
    async def main():
        print("ðŸš€ ETL Utilities - Quick Test")
        print("=" * 50)
        
        # Quick test
        print("\nðŸ“‹ Testing Connections...")
        test_results = await quick_test()
        print(json.dumps(test_results, indent=2, default=str))
        
        # Validate tables
        print("\nðŸ” Validating Tables...")
        validation_results = await validate_all_tables()
        print(f"System Health: {validation_results['system_health_score']}%")
        print(f"Healthy Tables: {validation_results['summary']['healthy_tables']}")
        print(f"Total Tables: {validation_results['summary']['total_tables']}")
    
    asyncio.run(main())

-- ./app/etl/config.py --
"""
ðŸŽ¯ ETL Configuration System - CASE MISMATCH FIXED
Fixed primary key names to match transformer output (lowercase)

ISSUE FIXED: Config used "ARCHIVO" but transformer outputs "archivo"  
ROOT CAUSE: Case mismatch between config primary_key and transformer output
"""

from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional


class ExtractionMode(str, Enum):
    """Extraction modes for different scenarios"""
    INCREMENTAL = "incremental"        # Default: Extract only new/changed data
    FULL_REFRESH = "full_refresh"      # Force: Complete table refresh
    SLIDING_WINDOW = "sliding_window"  # Window: Re-process last N days


class TableType(str, Enum):
    """Table types for different dashboard purposes"""
    DIMENSION = "dimansion"
    DASHBOARD = "dashboard"        # Main dashboard aggregation
    EVOLUTION = "evolution"        # Time series data
    ASSIGNMENT = "assignment"      # Monthly comparisons
    OPERATION = "operation"        # Hourly operations data
    PRODUCTIVITY = "productivity"  # Agent performance data


@dataclass
class ExtractionConfig:
    """Configuration for a specific table extraction"""
    
    # Table identification
    table_name: str
    table_type: TableType
    description: str
    
    # Primary key configuration - âœ… CASE FIXED FOR TRANSFORMER OUTPUT
    primary_key: List[str]
    incremental_column: Optional[str]
    
    # Extraction strategy
    default_mode: ExtractionMode = ExtractionMode.INCREMENTAL
    lookback_days: int = 7  # Days to re-process for data quality
    batch_size: int = 10000  # Records per batch
    
    # BigQuery specific
    source_dataset: str = "BI_USA"
    source_table: str = None
    
    # Quality checks
    required_columns: List[str] = None
    min_expected_records: int = 0
    
    # Scheduling
    refresh_frequency_hours: int = 6  # How often to refresh
    max_execution_time_minutes: int = 30  # Timeout


class ETLConfig:
    """
    Centralized ETL configuration for Pulso Dashboard - CASE MISMATCH FIXED
    
    STRATEGY: Primary key names now match transformer output (lowercase)
    FIXED: All primary keys use lowercase to match transformer dict keys
    """
    
    # ðŸŒŸ PROJECT CONFIGURATION
    PROJECT_ID = "mibot-222814"
    DATASET = "BI_USA"
    
    # ðŸ”„ RAW SOURCE CONFIGURATIONS - âœ… CASE FIXED TO MATCH TRANSFORMER
    EXTRACTION_CONFIGS: Dict[str, ExtractionConfig] = {
        
        # ðŸ“… CALENDARIO - âœ… PRIMARY KEY NAMES FIXED TO LOWERCASE
        "raw_calendario": ExtractionConfig(
            table_name="raw_calendario",
            table_type=TableType.DASHBOARD,
            description="Campaign calendar",
            primary_key=["archivo", "periodo_date"],  # âœ… FIXED: lowercase to match transformer
            incremental_column="fecha_apertura",
            source_table="bi_P3fV4dWNeMkN5RJMhV8e_dash_calendario_v5",
            lookback_days=7,
            required_columns=["archivo", "fecha_apertura"],  # âœ… FIXED: lowercase
            min_expected_records=1
        ),
        
        # ðŸ‘¥ ASIGNACIONES - âœ… PRIMARY KEY NAMES FIXED TO LOWERCASE
        "raw_asignaciones": ExtractionConfig(
            table_name="raw_asignaciones",
            table_type=TableType.ASSIGNMENT,
            description="Client assignments",
            primary_key=["cod_luna", "cuenta", "archivo", "fecha_asignacion"],  # âœ… FIXED: all lowercase
            incremental_column="creado_el",
            source_table="batch_P3fV4dWNeMkN5RJMhV8e_asignacion",
            lookback_days=30,
            batch_size=50000,
            required_columns=["cod_luna", "cuenta", "archivo"],  # âœ… FIXED: lowercase
            min_expected_records=1
        ),
        
        # ðŸ’° TRANDEUDA - âœ… PRIMARY KEY NAMES FIXED TO LOWERCASE
        "raw_trandeuda": ExtractionConfig(
            table_name="raw_trandeuda", 
            table_type=TableType.DASHBOARD,
            description="Daily debt snapshots",
            primary_key=["cod_cuenta", "nro_documento", "archivo", "fecha_proceso"],  # âœ… FIXED: lowercase
            incremental_column="creado_el",
            source_table="batch_P3fV4dWNeMkN5RJMhV8e_tran_deuda",
            lookback_days=14,
            batch_size=100000,
            required_columns=["cod_cuenta", "monto_exigible"],  # âœ… FIXED: lowercase
            min_expected_records=1
        ),
        
        # ðŸ’³ PAGOS - âœ… PRIMARY KEY NAMES ALREADY LOWERCASE (correct)
        "raw_pagos": ExtractionConfig(
            table_name="raw_pagos",
            table_type=TableType.DASHBOARD,
            description="Payment transactions", 
            primary_key=["nro_documento", "fecha_pago", "monto_cancelado"],  # âœ… Already correct
            incremental_column="creado_el",
            source_table="batch_P3fV4dWNeMkN5RJMhV8e_pagos",
            lookback_days=30,
            batch_size=25000,
            required_columns=["nro_documento", "fecha_pago", "monto_cancelado"],  # âœ… lowercase
            min_expected_records=1
        ),
        
        # ðŸŽ¯ GESTIONES UNIFICADAS - âœ… PRIMARY KEY NAMES ALREADY LOWERCASE (correct)
        "gestiones_unificadas": ExtractionConfig(
            table_name="gestiones_unificadas",
            table_type=TableType.OPERATION,
            description="Unified gestiones view",
            primary_key=["cod_luna", "timestamp_gestion"],  # âœ… Already correct
            incremental_column="timestamp_gestion",
            source_table="bi_P3fV4dWNeMkN5RJMhV8e_vw_gestiones_unificadas", 
            lookback_days=3,
            batch_size=75000,
            refresh_frequency_hours=1,
            required_columns=["cod_luna", "fecha_gestion"],  # âœ… lowercase
            min_expected_records=1
        ),

        # ðŸ‘¨â€ðŸ’¼ HOMOLOGACION AGENTES (MIBOTAIR)
        "raw_homologacion_mibotair": ExtractionConfig(
            table_name="raw_homologacion_mibotair",
            table_type=TableType.DIMENSION,
            description="Homologation rules for human agent interactions (MibotAir)",
            primary_key=["n_1", "n_2", "n_3"],
            incremental_column=None,  # This is a dimension table, always full load
            source_table="homologacion_P3fV4dWNeMkN5RJMhV8e_v2",
            default_mode=ExtractionMode.FULL_REFRESH,
            refresh_frequency_hours=24,  # Refreshed daily
        ),

        # ðŸ¤– HOMOLOGACION VOICEBOT
        "raw_homologacion_voicebot": ExtractionConfig(
            table_name="raw_homologacion_voicebot",
            table_type=TableType.DIMENSION,
            description="Homologation rules for Voicebot interactions",
            primary_key=["bot_management", "bot_sub_management", "bot_compromiso"],
            incremental_column=None,
            source_table="homologacion_P3fV4dWNeMkN5RJMhV8e_voicebot",
            default_mode=ExtractionMode.FULL_REFRESH,
            refresh_frequency_hours=24,
        ),

        # ðŸ¤µ EJECUTIVOS (AGENTES)
        "raw_ejecutivos": ExtractionConfig(
            table_name="raw_ejecutivos",
            table_type=TableType.DIMENSION,
            description="Agent and executive information, mapping email to document ID",
            primary_key=["correo_name"],
            incremental_column=None,
            source_table="sync_mibotair_batch_SYS_user",  # Filtered by client_id in query
            default_mode=ExtractionMode.FULL_REFRESH,
            refresh_frequency_hours=24,
        )
    }
    
    # ðŸŽ¯ QUERY TEMPLATES - Using REAL BigQuery field names
    EXTRACTION_QUERY_TEMPLATES: Dict[str, str] = {
        
        # ðŸ“… CALENDARIO - âœ… REAL SCHEMA
        "raw_calendario": """
        SELECT 
            ARCHIVO,                           -- âœ… Real field name
            TIPO_CARTERA,                      -- âœ… Real field name
            fecha_apertura,                    -- âœ… Real field name
            fecha_trandeuda,                   -- âœ… Real field name
            fecha_cierre,                      -- âœ… Real field name
            FECHA_CIERRE_PLANIFICADA,          -- âœ… Real field name
            DURACION_CAMPANA_DIAS_HABILES,     -- âœ… Real field name
            ANNO_ASIGNACION,                   -- âœ… Real field name (NOT FECHA_ASIGNACION)
            PERIODO_ASIGNACION,                -- âœ… Real field name
            ES_CARTERA_ABIERTA,                -- âœ… Real field name
            RANGO_VENCIMIENTO,                 -- âœ… Real field name
            ESTADO_CARTERA,                    -- âœ… Real field name
            periodo_mes,                       -- âœ… Real field name
            periodo_date,                      -- âœ… Real field name (partition column)
            tipo_ciclo_campana,                -- âœ… Real field name
            categoria_duracion,                -- âœ… Real field name
            CURRENT_TIMESTAMP() as extraction_timestamp
        FROM `mibot-222814.BI_USA.bi_P3fV4dWNeMkN5RJMhV8e_dash_calendario_v5`
        WHERE {incremental_filter}
        """,
        
        # ðŸ‘¥ ASIGNACIONES - âœ… REAL SCHEMA
        "raw_asignaciones": """
        SELECT 
            CAST(cliente AS STRING) as cliente,        -- âœ… Real field name (INT64 â†’ STRING)
            CAST(cuenta AS STRING) as cuenta,          -- âœ… Real field name (INT64 â†’ STRING)
            CAST(cod_luna AS STRING) as cod_luna,      -- âœ… Real field name (INT64 â†’ STRING)
            CAST(telefono AS STRING) as telefono,      -- âœ… Real field name (INT64 â†’ STRING)
            tramo_gestion,                             -- âœ… Real field name
            min_vto,                                   -- âœ… Real field name
            negocio,                                   -- âœ… Real field name
            dias_sin_trafico,                          -- âœ… Real field name
            decil_contacto,                            -- âœ… Real field name
            decil_pago,                                -- âœ… Real field name
            zona,                                      -- âœ… Real field name
            rango_renta,                               -- âœ… Real field name
            campania_act,                              -- âœ… Real field name
            archivo,                                   -- âœ… Real field name
            creado_el,                                 -- âœ… Real field name
            DATE(creado_el) as fecha_asignacion,       -- âœ… Derived from creado_el (partition column)
            CURRENT_TIMESTAMP() as extraction_timestamp
        FROM `mibot-222814.BI_USA.batch_P3fV4dWNeMkN5RJMhV8e_asignacion`
        WHERE {incremental_filter}
        """,
        
        # ðŸ’° TRANDEUDA - âœ… REAL SCHEMA
        "raw_trandeuda": """
        SELECT 
            cod_cuenta,                                -- âœ… Real field name (STRING, not INT64)
            nro_documento,                             -- âœ… Real field name
            fecha_vencimiento,                         -- âœ… Real field name
            monto_exigible,                            -- âœ… Real field name (FLOAT64)
            archivo,                                   -- âœ… Real field name
            creado_el,                                 -- âœ… Real field name
            DATE(creado_el) as fecha_proceso,          -- âœ… Derived from creado_el (partition column)
            motivo_rechazo,                            -- âœ… Real field name
            CURRENT_TIMESTAMP() as extraction_timestamp
        FROM `mibot-222814.BI_USA.batch_P3fV4dWNeMkN5RJMhV8e_tran_deuda`
        WHERE {incremental_filter}
          AND monto_exigible > 0
          AND (motivo_rechazo IS NULL OR motivo_rechazo = '')
        """,
        
        # ðŸ’³ PAGOS - âœ… REAL SCHEMA
        "raw_pagos": """
        SELECT 
            cod_sistema,                               -- âœ… Real field name (STRING)
            nro_documento,                             -- âœ… Real field name
            monto_cancelado,                           -- âœ… Real field name (FLOAT64)
            fecha_pago,                                -- âœ… Real field name (partition column)
            archivo,                                   -- âœ… Real field name
            creado_el,                                 -- âœ… Real field name
            motivo_rechazo,                            -- âœ… Real field name
            CURRENT_TIMESTAMP() as extraction_timestamp
        FROM `mibot-222814.BI_USA.batch_P3fV4dWNeMkN5RJMhV8e_pagos`
        WHERE {incremental_filter}
          AND monto_cancelado > 0
          AND (motivo_rechazo IS NULL OR motivo_rechazo = '')
        """,
        
        # ðŸŽ¯ GESTIONES UNIFICADAS - âœ… REAL SCHEMA FROM VIEW
        "gestiones_unificadas": """
        SELECT 
            CAST(cod_luna AS STRING) as cod_luna,      -- âœ… Real field name (INT64 â†’ STRING)
            fecha_gestion,                             -- âœ… Real field name (DATE)
            timestamp_gestion,                         -- âœ… Real field name (TIMESTAMP, partition column)
            canal_origen,                              -- âœ… Real field name ('BOT'|'HUMANO')
            management_original,                       -- âœ… Real field name
            sub_management_original,                   -- âœ… Real field name
            compromiso_original,                       -- âœ… Real field name
            nivel_1,                                   -- âœ… Real field name (homologated)
            nivel_2,                                   -- âœ… Real field name (homologated)
            contactabilidad,                           -- âœ… Real field name (homologated)
            es_contacto_efectivo,                      -- âœ… Real field name (BOOLEAN)
            es_contacto_no_efectivo,                   -- âœ… Real field name (BOOLEAN)
            es_compromiso,                             -- âœ… Real field name (BOOLEAN)
            peso_gestion,                              -- âœ… Real field name (INT64)
            CURRENT_TIMESTAMP() as extraction_timestamp
        FROM `mibot-222814.BI_USA.bi_P3fV4dWNeMkN5RJMhV8e_vw_gestiones_unificadas`
        WHERE {incremental_filter}
        """,
        # ðŸ‘¨â€ðŸ’¼ HOMOLOGACION AGENTES (MIBOTAIR) - âœ… Explicit Columns
        "raw_homologacion_mibotair": """
         SELECT management,
                n_1,
                n_2,
                n_3,
                peso,
                contactabilidad,
                tipo_gestion,
                codigo_rpta,
                pdp,
                gestor,
                CURRENT_TIMESTAMP() as extraction_timestamp
         FROM ` mibot-222814.BI_USA.homologacion_P3fV4dWNeMkN5RJMhV8e_v2 `
         WHERE {incremental_filter} -- This will be 1=1 for full refresh
         """,

        # ðŸ¤– HOMOLOGACION VOICEBOT - âœ… Explicit Columns
        "raw_homologacion_voicebot": """
         SELECT bot_management,
                bot_sub_management,
                bot_compromiso,
                n1_homologado,
                n2_homologado,
                n3_homologado,
                contactabilidad_homologada,
                es_pdp_homologado,
                peso_homologado,
                CURRENT_TIMESTAMP() as extraction_timestamp
         FROM ` mibot-222814.BI_USA.homologacion_P3fV4dWNeMkN5RJMhV8e_voicebot `
         WHERE {incremental_filter} -- This will be 1=1 for full refresh
         """,

        # ðŸ¤µ EJECUTIVOS (AGENTES) - âœ… Explicit Columns & Business Filter
        "raw_ejecutivos": """
          SELECT DISTINCT correo_name,
                          TRIM(nombre)        as nombre,
                          document,
                          CURRENT_TIMESTAMP() as extraction_timestamp
          FROM ` mibot-222814.BI_USA.sync_mibotair_batch_SYS_user `
          WHERE id_cliente = 145
            AND {incremental_filter} -- This will be 1=1 for full refresh
          """
    }
    
    # ðŸš¨ GLOBAL SETTINGS
    DEFAULT_TIMEZONE = "America/Lima"
    MAX_RETRY_ATTEMPTS = 3
    RETRY_DELAY_SECONDS = 30
    
    @classmethod
    def get_config(cls, table_name: str) -> ExtractionConfig:
        """Get configuration for a specific table"""
        if table_name not in cls.EXTRACTION_CONFIGS:
            raise ValueError(f"No configuration found for table: {table_name}")
        return cls.EXTRACTION_CONFIGS[table_name]
    
    @classmethod
    def get_query_template(cls, table_name: str) -> str:
        """Get query template for a specific table"""
        if table_name not in cls.EXTRACTION_QUERY_TEMPLATES:
            raise ValueError(f"No query template found for table: {table_name}")
        return cls.EXTRACTION_QUERY_TEMPLATES[table_name]
    
    @classmethod
    def get_query(cls, table_name: str, since_date: datetime = None) -> str:
        """
        Get formatted extraction query for a specific table
        
        Args:
            table_name: Name of the table
            since_date: Extract data since this date (None for full refresh)
            
        Returns:
            Formatted SQL query ready for BigQuery execution
        """
        template = cls.get_query_template(table_name)
        
        if since_date is None:
            # Full refresh - no filter
            incremental_filter = "1=1"  # Always true
        else:
            # Incremental - use date filter
            incremental_filter = cls.get_incremental_filter(table_name, since_date)
        
        # Format the template with the actual filter
        formatted_query = template.format(incremental_filter=incremental_filter)
        
        return formatted_query.strip()
    
    @classmethod
    def get_incremental_filter(cls, table_name: str, since_date: datetime) -> str:
        """
        Generate incremental filter for a specific table
        
        Args:
            table_name: Name of the table
            since_date: Extract data since this date
            
        Returns:
            SQL WHERE clause for incremental extraction
        """
        config = cls.get_config(table_name)

        if not config.incremental_column:
            return "1=1"  # Devuelve un filtro que no hace nada
        
        # Apply lookback window for data quality
        lookback_date = since_date - timedelta(days=config.lookback_days)
        
        # Generate filters based on table type
        if table_name == "raw_calendario":
            # Use business date: fecha_apertura
            return f"fecha_apertura >= '{lookback_date.strftime('%Y-%m-%d')}'"
        elif table_name in ["raw_asignaciones", "raw_trandeuda", "raw_pagos"]:
            # Use technical date: creado_el (we'll add filename date logic later)
            return f"DATE(creado_el) >= '{lookback_date.strftime('%Y-%m-%d')}'"
        elif table_name == "gestiones_unificadas":
            # Use business timestamp: timestamp_gestion
            return f"DATE(timestamp_gestion) >= '{lookback_date.strftime('%Y-%m-%d')}'"
        else:
            # Default fallback
            return f"DATE(creado_el) >= '{lookback_date.strftime('%Y-%m-%d')}'"
    
    @classmethod
    def list_tables(cls) -> List[str]:
        """List all configured tables"""
        return list(cls.EXTRACTION_CONFIGS.keys())
    
    @classmethod
    def get_dashboard_tables(cls) -> List[str]:
        """Get core tables needed for dashboard calculation"""
        return [
            "raw_calendario",           # Campaign definitions
            "raw_asignaciones",         # Client assignments
            "raw_trandeuda",           # Debt snapshots
            "raw_pagos",               # Payments
            "gestiones_unificadas"     # All gestiones with homologation
        ]
    
    @classmethod
    def get_raw_source_tables(cls) -> List[str]:
        """Get all raw source tables for initial extraction"""
        return [name for name in cls.EXTRACTION_CONFIGS.keys() if name.startswith("raw_")]


# ðŸŽ¯ CONVENIENCE CONSTANTS FOR EASY IMPORTS
DASHBOARD_TABLES = ETLConfig.get_dashboard_tables()
RAW_SOURCE_TABLES = ETLConfig.get_raw_source_tables()
ALL_TABLES = ETLConfig.list_tables()

# Default extraction configuration
DEFAULT_CONFIG = ExtractionConfig(
    table_name="default",
    table_type=TableType.DASHBOARD,
    description="Default configuration",
    primary_key=["id"],
    incremental_column="updated_at",
    lookback_days=1
)

-- ./app/etl/extractors/bigquery_extractor.py --
"""
ðŸš€ Production BigQuery Extractor - FIXED QueryJob.num_rows ISSUE
Intelligent incremental extraction with robust BigQuery row handling

FIXED: QueryJob.num_rows AttributeError - use query_job.result().total_rows instead
ADDED: Better error handling and row processing
"""

import asyncio
import time
import uuid
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, AsyncGenerator

import pandas as pd
from google.auth import default
from google.cloud import bigquery
from google.cloud.exceptions import GoogleCloudError

from app.core.logging import LoggerMixin
from app.etl.config import ETLConfig, ExtractionMode
from app.etl.watermarks import get_watermark_manager, WatermarkManager


class BigQueryExtractor(LoggerMixin):
    """
    Production-ready BigQuery extractor for incremental data extraction
    
    FIXED: QueryJob.num_rows AttributeError
    """
    
    def __init__(self, project_id: str = None):
        super().__init__()
        self.project_id = project_id or ETLConfig.PROJECT_ID
        self.client: Optional[bigquery.Client] = None
        self.watermark_manager: Optional[WatermarkManager] = None
        
        # Performance settings
        self.max_retries = ETLConfig.MAX_RETRY_ATTEMPTS
        self.retry_delay = ETLConfig.RETRY_DELAY_SECONDS
        self.default_timeout = 300  # 5 minutes default query timeout
        
    async def _ensure_client(self) -> bigquery.Client:
        """Ensure BigQuery client is initialized"""
        if self.client is None:
            # Initialize with default credentials
            credentials, _ = default()
            self.client = bigquery.Client(
                project=self.project_id,
                credentials=credentials
            )
            self.logger.info(f"BigQuery client initialized for project: {self.project_id}")
        
        return self.client
    
    async def _ensure_watermark_manager(self) -> WatermarkManager:
        """Ensure watermark manager is initialized"""
        if self.watermark_manager is None:
            self.watermark_manager = await get_watermark_manager()
        return self.watermark_manager
    
    def _serialize_bigquery_row(self, row) -> Dict[str, Any]:
        """
        Safely convert BigQuery row to dictionary with proper serialization
        
        FIXED: Handles BigQuery row objects correctly
        """
        try:
            # Try different approaches to convert row to dict
            if hasattr(row, '_fields'):
                # Named tuple style (most common)
                row_dict = dict(zip(row._fields, row))
            elif hasattr(row, 'items'):
                # Dict-like object
                row_dict = dict(row.items())
            elif hasattr(row, 'keys') and hasattr(row, 'values'):
                # Row with keys/values methods
                row_dict = dict(zip(row.keys(), row.values()))
            else:
                # Fallback: try direct dict conversion
                row_dict = dict(row)
            
            # Handle datetime and other special types
            serialized_dict = {}
            for key, value in row_dict.items():
                if isinstance(value, datetime):
                    serialized_dict[key] = value.isoformat()
                elif isinstance(value, (list, tuple)):
                    # Handle arrays/repeated fields
                    serialized_dict[key] = list(value) if value else []
                elif value is None:
                    serialized_dict[key] = None
                else:
                    # Keep as is for primitives (str, int, float, bool)
                    serialized_dict[key] = value
            
            return serialized_dict
            
        except Exception as e:
            self.logger.error(f"Failed to serialize BigQuery row: {str(e)}")
            self.logger.debug(f"Row type: {type(row)}, Row attributes: {dir(row)}")
            raise ValueError(f"Cannot serialize BigQuery row: {str(e)}")
    
    async def _execute_query_streaming(
        self, 
        query: str, 
        batch_size: int = 10000
    ) -> AsyncGenerator[List[Dict[str, Any]], None]:
        """
        Execute BigQuery query and yield results in batches
        
        FIXED: QueryJob.num_rows AttributeError and error handling
        """
        client = await self._ensure_client()
        
        try:
            # Configure query job
            job_config = bigquery.QueryJobConfig(
                use_query_cache=True,
                maximum_bytes_billed=10**10,  # 10GB limit for safety
            )
            
            # Start query job
            self.logger.info(f"Starting BigQuery job for batch size {batch_size}")
            query_job = client.query(query, job_config=job_config)
            
            # Wait for job to complete with timeout
            query_result = query_job.result(timeout=self.default_timeout)
            
            # FIXED: Get total rows from query result, not query job
            try:
                total_rows = query_result.total_rows if hasattr(query_result, 'total_rows') else 'unknown'
                self.logger.debug(f"Query job completed. Total rows: {total_rows}")
            except Exception as e:
                self.logger.debug(f"Could not get total rows count: {str(e)}")
            
            # Stream results in batches
            total_processed = 0
            batch_count = 0
            
            # Process results in pages/batches
            for page in query_result.pages:
                batch_data = []
                for row in page:
                    try:
                        # Use improved row serialization
                        row_dict = self._serialize_bigquery_row(row)
                        batch_data.append(row_dict)
                        
                    except Exception as e:
                        self.logger.error(f"Failed to process row: {str(e)}")
                        # Log row details for debugging
                        self.logger.debug(f"Problematic row type: {type(row)}")
                        # Skip this row and continue
                        continue
                
                if batch_data:
                    total_processed += len(batch_data)
                    batch_count += 1
                    
                    self.logger.debug(f"Yielding batch {batch_count} with {len(batch_data)} rows")
                    yield batch_data
                    
                    # Respect batch size limits
                    if len(batch_data) >= batch_size:
                        break
            
            self.logger.info(f"Query completed: {total_processed} rows processed in {batch_count} batches")
            
        except GoogleCloudError as e:
            self.logger.error(f"BigQuery error: {str(e)}")
            raise
        except Exception as e:
            self.logger.error(f"Unexpected error in query execution: {str(e)}")
            raise
    
    async def extract_table_incremental(
        self, 
        table_name: str,
        mode: ExtractionMode = ExtractionMode.INCREMENTAL,
        since_date: Optional[datetime] = None,
        force: bool = False
    ) -> AsyncGenerator[List[Dict[str, Any]], None]:
        """
        Extract data for a specific table incrementally
        
        FIXED: Query generation compatibility
        """
        # Get configuration
        config = ETLConfig.get_config(table_name)
        
        # Generate extraction ID for tracking
        extraction_id = str(uuid.uuid4())
        
        watermark_manager = await self._ensure_watermark_manager()
        
        try:
            # Start extraction tracking
            await watermark_manager.start_extraction(table_name, extraction_id)
            
            # Determine since_date if not provided
            if since_date is None and mode == ExtractionMode.INCREMENTAL:
                last_extracted = await watermark_manager.get_last_extraction_time(table_name)
                if last_extracted and not force:
                    # Check if extraction is recent enough
                    hours_since_last = (datetime.now(timezone.utc) - last_extracted).total_seconds() / 3600
                    if hours_since_last < config.refresh_frequency_hours:
                        self.logger.info(
                            f"Skipping {table_name} - last extracted {hours_since_last:.1f}h ago "
                            f"(frequency: {config.refresh_frequency_hours}h)"
                        )
                        return
                    
                    since_date = last_extracted
            
            # FIXED: Use the correct method to get formatted query
            final_query = ETLConfig.get_query(table_name, since_date)
            
            self.logger.info(
                f"Starting extraction for {table_name} "
                f"(mode: {mode}, since: {since_date}, ID: {extraction_id})"
            )
            
            self.logger.debug(f"Executing query for {table_name}:\\n{final_query}")
            
            start_time = time.time()
            total_records = 0
            
            # Stream data in batches
            async for batch in self._execute_query_streaming(
                final_query, 
                config.batch_size
            ):
                total_records += len(batch)
                yield batch
            
            # Update watermark on success
            duration = time.time() - start_time
            await watermark_manager.update_watermark(
                table_name=table_name,
                timestamp=datetime.now(timezone.utc),
                records_extracted=total_records,
                extraction_duration_seconds=duration,
                status="success",
                extraction_id=extraction_id,
                metadata={
                    "mode": mode.value,
                    "since_date": since_date.isoformat() if since_date else None,
                    "batch_size": config.batch_size,
                    "force": force
                }
            )
            
            self.logger.info(
                f"Extraction completed for {table_name}: "
                f"{total_records} records in {duration:.2f}s"
            )
            
        except Exception as e:
            # Update watermark on failure
            await watermark_manager.update_watermark(
                table_name=table_name,
                timestamp=datetime.now(timezone.utc),
                status="failed",
                error_message=str(e),
                extraction_id=extraction_id
            )
            
            self.logger.error(f"Extraction failed for {table_name}: {str(e)}")
            raise
    
    async def extract_table_full(
        self, 
        table_name: str
    ) -> List[Dict[str, Any]]:
        """
        Extract complete table data (for smaller tables)
        
        Returns all data in memory - use carefully!
        """
        all_data = []
        
        async for batch in self.extract_table_incremental(
            table_name=table_name,
            mode=ExtractionMode.FULL_REFRESH
        ):
            all_data.extend(batch)
        
        return all_data
    
    async def extract_multiple_tables(
        self, 
        table_names: List[str],
        mode: ExtractionMode = ExtractionMode.INCREMENTAL,
        max_concurrent: int = 3
    ) -> Dict[str, int]:
        """
        Extract multiple tables concurrently
        
        Args:
            table_names: List of table names to extract
            mode: Extraction mode for all tables  
            max_concurrent: Maximum concurrent extractions
            
        Returns:
            Dictionary with table_name -> record_count
        """
        semaphore = asyncio.Semaphore(max_concurrent)
        results = {}
        
        async def extract_single(table_name: str) -> int:
            async with semaphore:
                total_records = 0
                try:
                    async for batch in self.extract_table_incremental(table_name, mode):
                        total_records += len(batch)
                    return total_records
                except Exception as e:
                    self.logger.error(f"Failed to extract {table_name}: {str(e)}")
                    return -1  # Indicate failure
        
        # Start all extractions
        tasks = []
        for table_name in table_names:
            task = asyncio.create_task(extract_single(table_name))
            tasks.append((table_name, task))
        
        # Wait for completion
        for table_name, task in tasks:
            try:
                record_count = await task
                results[table_name] = record_count
                self.logger.info(f"Completed {table_name}: {record_count} records")
            except Exception as e:
                results[table_name] = -1
                self.logger.error(f"Failed {table_name}: {str(e)}")
        
        return results

    # En la clase BigQueryExtractor, aÃ±Ã¡delo despuÃ©s de extract_multiple_tables

    async def run_custom_query_to_list(self, query: str) -> list[dict]:
        """
        Executes an arbitrary SQL query and returns all results in a single list.

        NOTE: Use with caution, as this loads all results into memory.
        Ideal for the CalendarDrivenCoordinator which processes one campaign at a time.

        Args:
            query: The SQL query string to execute.

        Returns:
            A list of dictionaries containing all rows from the query.
        """
        self.logger.info(f"Running custom query and collecting all results into memory...")

        all_results = []
        # Usamos el eficiente mÃ©todo de streaming interno para obtener los datos
        async for batch in self._execute_query_streaming(query):
            all_results.extend(batch)

        self.logger.info(f"Custom query collected {len(all_results)} records.")
        return all_results

        # En: app/etl/extractors/bigquery_extractor.py
        # Dentro de la clase BigQueryExtractor

    async def stream_custom_query(
            self,
            query: str,
            batch_size: int = 10000
    ) -> AsyncGenerator[List[Dict[str, Any]], None]:
        """
        Executes an arbitrary SQL query and yields results in batches.
        This is the preferred memory-efficient method for large queries.

        Args:
            query: The SQL query string to execute.
            batch_size: The number of rows to fetch per batch.

        Yields:
            A batch of data as a list of dictionaries.
        """
        self.logger.info(f"Streaming custom query results with batch size {batch_size}...")
        self.logger.debug(f"Query Snippet: {query[:500]}...")

        # This method simply acts as a public entrypoint to the
        # internal streaming logic that is already well-implemented.
        async for batch in self._execute_query_streaming(query, batch_size):
            yield batch
    
    async def test_query(self, query: str) -> Dict[str, Any]:
        """
        Test a query without full execution (for validation)
        
        Returns query metadata and sample results
        """
        client = await self._ensure_client()
        
        try:
            # Add LIMIT to avoid large results in test
            test_query = f"SELECT * FROM ({query}) LIMIT 10"
            
            job_config = bigquery.QueryJobConfig(
                use_query_cache=True,
                dry_run=False  # We want actual results for testing
            )
            
            start_time = time.time()
            query_job = client.query(test_query, job_config=job_config)
            results = query_job.result(timeout=30)  # Short timeout for test
            
            # Get sample data using improved serialization
            sample_data = []
            for row in results:
                try:
                    serialized_row = self._serialize_bigquery_row(row)
                    sample_data.append(serialized_row)
                except Exception as e:
                    self.logger.warning(f"Failed to serialize sample row: {str(e)}")
            
            return {
                "status": "success",
                "execution_time_seconds": time.time() - start_time,
                "sample_records": len(sample_data),
                "schema": [
                    {"name": field.name, "type": field.field_type} 
                    for field in query_job.schema
                ],
                "sample_data": sample_data[:3],  # First 3 rows
                "total_bytes_processed": query_job.total_bytes_processed,
                "job_id": query_job.job_id
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "query": test_query
            }
    
    async def get_table_info(self, table_name: str) -> Dict[str, Any]:
        """Get metadata about a configured table"""
        try:
            config = ETLConfig.get_config(table_name)
            watermark_manager = await self._ensure_watermark_manager()
            watermark = await watermark_manager.get_watermark(table_name)
            
            return {
                "table_name": table_name,
                "table_type": config.table_type.value,
                "description": config.description,
                "primary_key": config.primary_key,
                "incremental_column": config.incremental_column,
                "lookback_days": config.lookback_days,
                "batch_size": config.batch_size,
                "refresh_frequency_hours": config.refresh_frequency_hours,
                "last_extraction": {
                    "timestamp": watermark.last_extracted_at.isoformat() if watermark else None,
                    "status": watermark.last_extraction_status if watermark else "never",
                    "records": watermark.records_extracted if watermark else 0,
                    "duration": watermark.extraction_duration_seconds if watermark else 0
                } if watermark else None
            }
            
        except Exception as e:
            return {
                "table_name": table_name,
                "error": str(e)
            }
    
    # ðŸŽ¯ Convenience functions for easy imports
async def get_extractor() -> BigQueryExtractor:
    """Get configured BigQuery extractor instance"""
    return BigQueryExtractor()


async def quick_extract(table_name: str) -> List[Dict[str, Any]]:
    """Quick extraction for small tables (returns all data)"""
    extractor = await get_extractor()
    return await extractor.extract_table_full(table_name)


async def extract_dashboard_tables() -> Dict[str, int]:
    """Extract all dashboard-related tables"""
    extractor = await get_extractor()
    dashboard_tables = ETLConfig.get_dashboard_tables()
    
    return await extractor.extract_multiple_tables(
        table_names=dashboard_tables,
        mode=ExtractionMode.INCREMENTAL,
        max_concurrent=2  # Conservative for dashboard tables
    )

-- ./app/etl/pipelines/extraction_pipeline.py --
"""
ðŸš€ Production ETL Pipeline for Pulso Dashboard
Orchestrates incremental extraction, transformation, and loading

Features:
- End-to-end ETL pipeline with data transformation
- Support for single table or multi-table operations
- Comprehensive monitoring and status reporting
- Production-ready transaction management
- FIXED: Now supports all table types (raw + mart + business logic)
"""

import asyncio
import uuid
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, AsyncGenerator
import logging
import time
from contextlib import asynccontextmanager

from app.etl.config import ETLConfig, ExtractionConfig, ExtractionMode
from app.etl.extractors.bigquery_extractor import BigQueryExtractor
from app.etl.transformers.unified_transformer import get_unified_transformer_registry, UnifiedTransformerRegistry
from app.etl.loaders.postgres_loader import PostgresLoader, LoadResult
from app.etl.watermarks import get_watermark_manager, WatermarkManager
from app.core.logging import LoggerMixin


class PipelineResult:
    """Result of a complete ETL pipeline execution"""
    
    def __init__(self, pipeline_id: str):
        self.pipeline_id = pipeline_id
        self.start_time = datetime.now(timezone.utc)
        self.end_time: Optional[datetime] = None
        self.status = "running"
        self.tables_processed: List[str] = []
        self.tables_failed: List[str] = []
        self.load_results: Dict[str, LoadResult] = {}
        self.transformation_stats: Dict[str, Dict[str, int]] = {}
        self.total_records_processed = 0
        self.total_records_transformed = 0
        self.error_message: Optional[str] = None
        self.metadata: Dict[str, Any] = {}
    
    def mark_completed(self, status: str = "success", error: str = None):
        """Mark pipeline as completed"""
        self.end_time = datetime.now(timezone.utc)
        self.status = status
        if error:
            self.error_message = error
    
    @property
    def duration_seconds(self) -> float:
        """Get pipeline duration in seconds"""
        end = self.end_time or datetime.now(timezone.utc)
        return (end - self.start_time).total_seconds()
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            "pipeline_id": self.pipeline_id,
            "start_time": self.start_time.isoformat(),
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "duration_seconds": self.duration_seconds,
            "status": self.status,
            "tables_processed": self.tables_processed,
            "tables_failed": self.tables_failed,
            "total_records_processed": self.total_records_processed,
            "total_records_transformed": self.total_records_transformed,
            "transformation_stats": self.transformation_stats,
            "load_results": {
                table: {
                    "total_records": result.total_records,
                    "inserted_records": result.inserted_records,
                    "updated_records": result.updated_records,
                    "skipped_records": result.skipped_records,
                    "load_duration_seconds": result.load_duration_seconds,
                    "status": result.status,
                    "error_message": result.error_message
                }
                for table, result in self.load_results.items()
            },
            "error_message": self.error_message,
            "metadata": self.metadata
        }


class ETLPipeline(LoggerMixin):
    """
    Production ETL Pipeline for Pulso Dashboard
    
    Orchestrates the complete Extract-Transform-Load process with:
    - Intelligent incremental processing
    - Data transformation and validation  
    - Error handling and recovery
    - Comprehensive monitoring
    - Transaction consistency
    - FIXED: Full support for 3-layer transformation pipeline
    """
    
    def __init__(self):
        super().__init__()
        self.extractor: Optional[BigQueryExtractor] = None
        self.transformer: Optional[UnifiedTransformerRegistry] = None
        self.loader: Optional[PostgresLoader] = None
        self.watermark_manager: Optional[WatermarkManager] = None
        
        # Pipeline tracking
        self.active_pipelines: Dict[str, PipelineResult] = {}
        self.max_concurrent_pipelines = 3
    
    async def _ensure_components(self):
        """Ensure all pipeline components are initialized"""
        if self.extractor is None:
            self.extractor = BigQueryExtractor()
        
        if self.transformer is None:
            self.transformer = get_unified_transformer_registry()
            self.logger.info(f"âœ… Unified transformer initialized with {len(self.transformer.get_supported_tables())} supported tables")
        
        if self.loader is None:
            self.loader = PostgresLoader()
        
        if self.watermark_manager is None:
            self.watermark_manager = await get_watermark_manager()
    
    async def _transform_data_stream(
        self,
        table_name: str,
        raw_data_stream: AsyncGenerator[List[Dict[str, Any]], None]
    ) -> AsyncGenerator[List[Dict[str, Any]], None]:
        """
        Transform data stream with batch processing
        
        Args:
            table_name: Name of target table
            raw_data_stream: Stream of raw data batches from BigQuery
            
        Yields:
            Transformed data batches ready for PostgreSQL
        """
        async for raw_batch in raw_data_stream:
            if not raw_batch:
                continue
            
            try:
                # Transform the batch using unified transformer
                transformed_batch = self.transformer.transform_table_data(table_name, raw_batch)
                
                if transformed_batch:
                    self.logger.debug(
                        f"Transformed batch for {table_name}: "
                        f"{len(raw_batch)} raw â†’ {len(transformed_batch)} clean records"
                    )
                    yield transformed_batch
                
            except Exception as e:
                self.logger.error(f"Transformation error for {table_name}: {str(e)}")
                # Continue with next batch rather than failing entire stream
                continue
    
    async def extract_transform_and_load_table(
        self, 
        table_name: str,
        mode: ExtractionMode = ExtractionMode.INCREMENTAL,
        force: bool = False,
        pipeline_result: Optional[PipelineResult] = None
    ) -> LoadResult:
        """
        Extract, transform, and load a single table
        
        Args:
            table_name: Name of table to process
            mode: Extraction mode
            force: Force extraction even if recent
            pipeline_result: Parent pipeline result for tracking
            
        Returns:
            LoadResult with operation statistics
        """
        await self._ensure_components()
        
        config = ETLConfig.get_config(table_name)
        
        self.logger.info(
            f"Starting ETL for {table_name} "
            f"(mode: {mode}, force: {force})"
        )
        
        try:
            # Step 1: Extract data stream from BigQuery
            raw_data_stream = self.extractor.extract_table_incremental(
                table_name=table_name,
                mode=mode,
                force=force
            )
            
            # Step 2: Transform data stream
            transformed_data_stream = self._transform_data_stream(
                table_name=table_name,
                raw_data_stream=raw_data_stream
            )
            
            # Step 3: Load transformed data with streaming
            load_result = await self.loader.load_data_streaming(
                table_name=table_name,
                data_stream=transformed_data_stream,
                primary_key=config.primary_key,
                upsert=True
            )
            
            # Step 4: Collect transformation statistics
            transformation_stats = self.transformer.get_transformation_stats()
            
            # Track in pipeline result
            if pipeline_result:
                pipeline_result.load_results[table_name] = load_result
                pipeline_result.transformation_stats[table_name] = transformation_stats
                pipeline_result.total_records_processed += load_result.total_records
                pipeline_result.total_records_transformed += transformation_stats.get('raw_stats', {}).get('records_transformed', 0)
                
                if load_result.status == "success":
                    pipeline_result.tables_processed.append(table_name)
                else:
                    pipeline_result.tables_failed.append(table_name)
            
            self.logger.info(
                f"Completed ETL for {table_name}: "
                f"{transformation_stats.get('raw_stats', {}).get('records_processed', 0)} extracted â†’ "
                f"{transformation_stats.get('raw_stats', {}).get('records_transformed', 0)} transformed â†’ "
                f"{load_result.total_records} loaded ({load_result.status})"
            )
            
            return load_result
            
        except Exception as e:
            error_msg = f"Failed ETL for {table_name}: {str(e)}"
            self.logger.error(error_msg)
            
            # Create error result
            load_result = LoadResult(
                table_name=table_name,
                total_records=0,
                inserted_records=0,
                updated_records=0,
                skipped_records=0,
                load_duration_seconds=0,
                status="failed",
                error_message=error_msg
            )
            
            # Track in pipeline result
            if pipeline_result:
                pipeline_result.load_results[table_name] = load_result
                pipeline_result.tables_failed.append(table_name)
            
            return load_result
        
        finally:
            # Reset transformer stats for next table
            if self.transformer:
                self.transformer.reset_all_stats()
    
    async def run_incremental_pipeline(
        self, 
        table_names: Optional[List[str]] = None,
        mode: ExtractionMode = ExtractionMode.INCREMENTAL,
        force: bool = False,
        max_concurrent: int = 2
    ) -> PipelineResult:
        """
        Run incremental ETL pipeline for multiple tables
        
        Args:
            table_names: List of tables to process (None = all configured tables)
            mode: Extraction mode for all tables
            force: Force extraction even if recent
            max_concurrent: Maximum concurrent table processing
            
        Returns:
            PipelineResult with comprehensive statistics
        """
        pipeline_id = str(uuid.uuid4())
        result = PipelineResult(pipeline_id)
        
        # Track active pipeline
        self.active_pipelines[pipeline_id] = result
        
        try:
            # Determine tables to process
            if table_names is None:
                table_names = ETLConfig.list_tables()
            
            result.metadata = {
                "mode": mode.value,
                "force": force,
                "max_concurrent": max_concurrent,
                "total_tables": len(table_names),
                "etl_stages": ["extract", "transform", "load"],
                "transformer_info": self.transformer.get_transformation_stats() if self.transformer else {}
            }
            
            self.logger.info(
                f"Starting ETL pipeline {pipeline_id} "
                f"for {len(table_names)} tables (mode: {mode})"
            )
            
            # Process tables with concurrency control
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_table(table_name: str):
                async with semaphore:
                    return await self.extract_transform_and_load_table(
                        table_name=table_name,
                        mode=mode,
                        force=force,
                        pipeline_result=result
                    )
            
            # Start all tasks
            tasks = [process_table(table) for table in table_names]
            
            # Wait for completion
            await asyncio.gather(*tasks, return_exceptions=True)
            
            # Determine final status
            if result.tables_failed:
                if result.tables_processed:
                    result.mark_completed("partial_success")
                else:
                    result.mark_completed("failed", f"All {len(result.tables_failed)} tables failed")
            else:
                result.mark_completed("success")
            
            self.logger.info(
                f"ETL Pipeline {pipeline_id} completed: "
                f"{len(result.tables_processed)} success, "
                f"{len(result.tables_failed)} failed, "
                f"{result.total_records_transformed}/{result.total_records_processed} transformed, "
                f"{result.duration_seconds:.2f}s"
            )
            
        except Exception as e:
            error_msg = f"ETL Pipeline {pipeline_id} failed: {str(e)}"
            self.logger.error(error_msg)
            result.mark_completed("failed", error_msg)
        
        finally:
            # Remove from active pipelines
            if pipeline_id in self.active_pipelines:
                del self.active_pipelines[pipeline_id]
        
        return result
    
    async def run_dashboard_refresh(self, force: bool = False) -> PipelineResult:
        """
        Run refresh specifically for dashboard tables
        
        This is the main method called by the HTTP API
        
        Args:
            force: Force refresh even if recently updated
            
        Returns:
            PipelineResult for dashboard refresh
        """
        dashboard_tables = ETLConfig.get_dashboard_tables()
        
        return await self.run_incremental_pipeline(
            table_names=dashboard_tables,
            mode=ExtractionMode.INCREMENTAL,
            force=force,
            max_concurrent=2  # Conservative for dashboard tables
        )
    
    async def run_business_logic_processing(self, archivo: str) -> Dict[str, Any]:
        """
        Run business logic processing for a specific campaign
        
        Args:
            archivo: Campaign identifier
            
        Returns:
            Business logic processing result
        """
        await self._ensure_components()
        
        self.logger.info(f"ðŸŽ¯ Starting business logic processing for campaign: {archivo}")
        
        try:
            # Use the unified transformer's business logic capabilities
            result = await self.transformer.process_campaign_business_logic(archivo)
            
            self.logger.info(f"âœ… Business logic completed for {archivo}")
            
            return result
            
        except Exception as e:
            error_msg = f"âŒ Business logic failed for {archivo}: {str(e)}"
            self.logger.error(error_msg)
            raise
    
    async def run_full_refresh(
        self, 
        table_names: Optional[List[str]] = None
    ) -> PipelineResult:
        """
        Run full refresh for specified tables
        
        Args:
            table_names: Tables to refresh (None = all tables)
            
        Returns:
            PipelineResult for full refresh
        """
        return await self.run_incremental_pipeline(
            table_names=table_names,
            mode=ExtractionMode.FULL_REFRESH,
            force=True,  # Full refresh is always forced
            max_concurrent=1  # Sequential for full refresh
        )
    
    async def test_transformation(self, table_name: str, sample_size: int = 100) -> Dict[str, Any]:
        """
        Test transformation for a specific table with sample data
        
        Args:
            table_name: Name of table to test
            sample_size: Number of sample records to test
            
        Returns:
            Transformation test results
        """
        await self._ensure_components()
        
        try:
            # Get sample data from BigQuery
            config = ETLConfig.get_config(table_name)
            base_query = ETLConfig.get_query(table_name)
            
            # Modify query to get sample data
            sample_query = f"{base_query.replace('{incremental_filter}', '1=1')} LIMIT {sample_size}"
            
            # Test query execution
            test_result = await self.extractor.test_query(sample_query)
            
            if test_result.get("status") == "success" and test_result.get("sample_data"):
                # Test transformation
                raw_data = test_result["sample_data"]
                transformed_data = self.transformer.transform_table_data(table_name, raw_data)
                transformation_stats = self.transformer.get_transformation_stats()
                
                return {
                    "status": "success",
                    "table_name": table_name,
                    "sample_size": len(raw_data),
                    "raw_sample": raw_data[:2],  # First 2 raw records
                    "transformed_sample": transformed_data[:2],  # First 2 transformed records
                    "transformation_stats": transformation_stats,
                    "transformer_info": self.transformer.get_table_info(table_name),
                    "schema_mapping": {
                        "raw_fields": list(raw_data[0].keys()) if raw_data else [],
                        "transformed_fields": list(transformed_data[0].keys()) if transformed_data else []
                    }
                }
            else:
                return {
                    "status": "failed",
                    "error": "Failed to extract sample data",
                    "query_result": test_result
                }
                
        except Exception as e:
            return {
                "status": "failed",
                "table_name": table_name,
                "error": str(e)
            }
        
        finally:
            # Reset transformer stats
            if self.transformer:
                self.transformer.reset_all_stats()
    
    async def get_pipeline_status(self, pipeline_id: str) -> Optional[Dict[str, Any]]:
        """Get status of a running pipeline"""
        if pipeline_id in self.active_pipelines:
            return self.active_pipelines[pipeline_id].to_dict()
        return None
    
    async def list_active_pipelines(self) -> List[Dict[str, Any]]:
        """List all currently active pipelines"""
        return [result.to_dict() for result in self.active_pipelines.values()]
    
    async def get_extraction_summary(self) -> Dict[str, Any]:
        """Get summary of recent extractions across all tables"""
        await self._ensure_components()
        
        # Get watermark summary
        watermark_summary = await self.watermark_manager.get_extraction_summary()
        
        # Get table-specific info
        table_info = {}
        for table_name in ETLConfig.list_tables():
            try:
                info = await self.extractor.get_table_info(table_name)
                info['transformer_info'] = self.transformer.get_table_info(table_name)
                table_info[table_name] = info
            except Exception as e:
                table_info[table_name] = {"error": str(e)}
        
        return {
            "summary": watermark_summary,
            "tables": table_info,
            "active_pipelines": len(self.active_pipelines),
            "configured_tables": len(ETLConfig.list_tables()),
            "dashboard_tables": ETLConfig.get_dashboard_tables(),
            "transformation_support": self.transformer.get_supported_tables() if self.transformer else [],
            "business_logic_status": self.transformer.get_business_logic_status() if self.transformer else {}
        }
    
    async def cleanup_and_recover(self) -> Dict[str, Any]:
        """Clean up failed extractions and attempt recovery"""
        await self._ensure_components()
        
        # Get failed extractions
        failed_extractions = await self.watermark_manager.get_failed_extractions()
        
        recovery_attempts = []
        
        # Attempt to recover failed extractions
        for failed in failed_extractions:
            try:
                self.logger.info(f"Attempting recovery for {failed.table_name}")
                
                # Try incremental extraction with transformation
                load_result = await self.extract_transform_and_load_table(
                    table_name=failed.table_name,
                    mode=ExtractionMode.INCREMENTAL,
                    force=True
                )
                
                recovery_attempts.append({
                    "table_name": failed.table_name,
                    "status": load_result.status,
                    "records": load_result.total_records,
                    "error": load_result.error_message
                })
                
            except Exception as e:
                recovery_attempts.append({
                    "table_name": failed.table_name,
                    "status": "failed",
                    "error": str(e)
                })
        
        return {
            "recovery_attempts": recovery_attempts,
            "recovered_tables": [
                attempt["table_name"] 
                for attempt in recovery_attempts 
                if attempt["status"] == "success"
            ]
        }


# ðŸŽ¯ Global pipeline instance
_pipeline: Optional[ETLPipeline] = None

def get_pipeline() -> ETLPipeline:
    """Get singleton pipeline instance"""
    global _pipeline
    if _pipeline is None:
        _pipeline = ETLPipeline()
    return _pipeline


# ðŸš€ Convenience functions for HTTP API
async def trigger_dashboard_refresh(force: bool = False) -> Dict[str, Any]:
    """Trigger dashboard refresh - called by HTTP API"""
    pipeline = get_pipeline()
    result = await pipeline.run_dashboard_refresh(force=force)
    return result.to_dict()


async def trigger_table_refresh(
    table_name: str, 
    force: bool = False
) -> Dict[str, Any]:
    """Trigger refresh for a specific table"""
    pipeline = get_pipeline()
    
    load_result = await pipeline.extract_transform_and_load_table(
        table_name=table_name,
        mode=ExtractionMode.INCREMENTAL,
        force=force
    )
    
    return {
        "table_name": table_name,
        "result": load_result.__dict__
    }


async def trigger_business_logic_processing(archivo: str) -> Dict[str, Any]:
    """Trigger business logic processing for a campaign"""
    pipeline = get_pipeline()
    return await pipeline.run_business_logic_processing(archivo)


async def test_table_transformation(table_name: str) -> Dict[str, Any]:
    """Test transformation for a specific table"""
    pipeline = get_pipeline()
    return await pipeline.test_transformation(table_name)


async def get_etl_status() -> Dict[str, Any]:
    """Get comprehensive ETL status - called by HTTP API"""
    pipeline = get_pipeline()
    return await pipeline.get_extraction_summary()

-- ./app/etl/coordinators/calendar_driven_coordinator.py --
# app/etl/coordinators/calendar_driven_coordinator.py

"""
ðŸŽ¯ Production-Ready Calendar-Driven ETL Coordinator
Intelligent, incremental, and cancellable data loading based on campaign time windows.

STRATEGY: Use `raw_calendario` as the source of truth. For each campaign,
check a specific watermark. If the campaign is closed and was successfully
processed, skip it. Otherwise, load all its data. This makes daily runs
highly efficient.

FEATURES:
- Watermark integration to avoid reprocessing completed campaigns.
- Cancellable execution via an asyncio.Event.
- `force_refresh` parameter to override watermarks.
- Memory-efficient end-to-end streaming pipeline.
- Concurrent data loading for tables within a single campaign.
"""

from datetime import datetime, date, timedelta, timezone
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import asyncio

from app.core.logging import LoggerMixin
from app.etl.config import ETLConfig
from app.etl.extractors.bigquery_extractor import BigQueryExtractor
from app.etl.transformers.unified_transformer import get_unified_transformer_registry, UnifiedTransformerRegistry
from app.etl.loaders.postgres_loader import get_loader, PostgresLoader
from app.etl.watermarks import get_watermark_manager, WatermarkManager
from app.database.connection import get_database_manager, DatabaseManager


@dataclass
class CampaignWindow:
    # ... (el dataclass CampaignWindow no cambia, puedes mantener el que tienes)
    archivo: str
    fecha_apertura: date
    fecha_trandeuda: Optional[date]
    fecha_cierre: Optional[date]
    tipo_cartera: str
    estado_cartera: str

    # ... (todas las @property no cambian)
    @property
    def asignacion_window_start(self) -> date:
        return self.fecha_apertura - timedelta(days=7)

    @property
    def asignacion_window_end(self) -> date:
        return self.fecha_apertura + timedelta(days=30)

    @property
    def trandeuda_window_start(self) -> date:
        return self.fecha_trandeuda or self.fecha_apertura

    @property
    def trandeuda_window_end(self) -> date:
        return self.fecha_cierre or (datetime.now().date() + timedelta(days=1))

    @property
    def pagos_window_start(self) -> date:
        return self.fecha_apertura

    @property
    def pagos_window_end(self) -> date:
        end_date = self.fecha_cierre or datetime.now().date()
        return end_date + timedelta(days=30)

    @property
    def gestiones_window_start(self) -> date:
        return self.fecha_apertura

    @property
    def gestiones_window_end(self) -> date:
        return self.fecha_cierre or (datetime.now().date() + timedelta(days=1))


@dataclass
class CampaignLoadResult:
    # ... (el dataclass CampaignLoadResult no cambia)
    archivo: str
    status: str
    tables_loaded: Dict[str, int]
    errors: List[str]
    duration_seconds: float


class CalendarDrivenCoordinator(LoggerMixin):
    """
    Coordinates intelligent ETL loading with watermarks and cancellation.
    """

    def __init__(self):
        super().__init__()
        self.extractor: Optional[BigQueryExtractor] = None
        self.transformer: Optional[UnifiedTransformerRegistry] = None
        self.loader: Optional[PostgresLoader] = None
        self.db_manager: Optional[DatabaseManager] = None
        self.watermark_manager: Optional[WatermarkManager] = None

        # State for cancellation
        self._cancel_event = asyncio.Event()
        self._is_running = False

    async def _initialize_components(self):
        """Initialize all required ETL components."""
        if self.db_manager is None: self.db_manager = await get_database_manager()
        if self.watermark_manager is None: self.watermark_manager = await get_watermark_manager()
        if self.extractor is None: self.extractor = BigQueryExtractor()
        if self.transformer is None: self.transformer = get_unified_transformer_registry()
        if self.loader is None: self.loader = await get_loader()

    @staticmethod
    def get_watermark_name(campaign_archivo: str) -> str:
        """Standardize watermark names for campaigns."""
        return f"campaign__{campaign_archivo}"

    async def get_campaign_windows(self, limit: Optional[int] = None) -> List[CampaignWindow]:
        """Gets campaign windows from the 'raw_calendario' table."""
        await self._initialize_components()
        query = """
        SELECT 
            archivo,
            fecha_apertura,
            fecha_trandeuda,
            fecha_cierre,
            tipo_cartera,
            estado_cartera
        FROM raw_calendario 
        ORDER BY fecha_apertura
        """
        if limit: query += f" LIMIT {limit}"

        try:
            rows = await self.db_manager.execute_query(query, fetch="all")
            campaigns = [CampaignWindow(**dict(row)) for row in rows]
            self.logger.info(f"Found {len(campaigns)} campaign windows in calendario.")
            return campaigns
        except Exception as e:
            self.logger.error(f"Failed to get campaign windows: {e}", exc_info=True)
            return []

    async def load_campaign_data(
            self,
            campaign: CampaignWindow,
            tables: Optional[List[str]] = None,
    ) -> CampaignLoadResult:
        """Loads all relevant data for a specific campaign."""
        start_time = datetime.now()
        watermark_name = self.get_watermark_name(campaign.archivo)

        if tables is None:
            tables = ['raw_asignaciones', 'raw_trandeuda', 'raw_pagos', 'gestiones_unificadas']

        self.logger.info(f"ðŸ—“ï¸ Loading campaign '{campaign.archivo}'...")
        await self._initialize_components()

        # Mark as running in watermark system
        await self.watermark_manager.start_extraction(watermark_name, f"campaign_run_{start_time.isoformat()}")

        tasks = [self._load_campaign_table(campaign, table_name) for table_name in tables]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        tables_loaded, errors = {}, []
        for i, result in enumerate(results):
            table_name = tables[i]
            if isinstance(result, Exception):
                errors.append(f"{table_name}: {result}")
                tables_loaded[table_name] = 0
            else:
                tables_loaded[table_name] = result

        status = 'success' if not errors else ('partial_success' if any(tables_loaded.values()) else 'failed')
        duration = (datetime.now() - start_time).total_seconds()

        load_result = CampaignLoadResult(
            archivo=campaign.archivo, status=status,
            tables_loaded=tables_loaded, errors=errors, duration_seconds=duration
        )

        # Update watermark with the final status
        await self.watermark_manager.update_watermark(
            table_name=watermark_name, timestamp=datetime.now(timezone.utc),
            records_extracted=sum(load_result.tables_loaded.values()),
            extraction_duration_seconds=load_result.duration_seconds,
            status=load_result.status,
            error_message=", ".join(load_result.errors) or None
        )
        self.logger.info(f"âœ… Finished campaign '{campaign.archivo}' with status: {status}")
        return load_result

    async def _load_campaign_table(
        self,
        campaign: CampaignWindow,
        table_name: str,
        force_refresh: bool = False
    ) -> int:
        """
        Loads a specific table for a campaign using a full streaming pipeline.
        Returns the number of records loaded.
        """
        if table_name == 'raw_asignaciones':
            start_date, end_date = campaign.asignacion_window_start, campaign.asignacion_window_end
        elif table_name == 'raw_trandeuda':
            start_date, end_date = campaign.trandeuda_window_start, campaign.trandeuda_window_end
        elif table_name == 'raw_pagos':
            start_date, end_date = campaign.pagos_window_start, campaign.pagos_window_end
        elif table_name == 'gestiones_unificadas':
            start_date, end_date = campaign.gestiones_window_start, campaign.gestiones_window_end
        else:
            raise ValueError(f"Unknown table for campaign loading: {table_name}")

        custom_query = self._build_campaign_query(table_name, campaign, start_date, end_date)

        # REFACTORED: Full end-to-end streaming pipeline

        # 1. Extractor Stream: Get an async generator of data from BigQuery
        raw_data_stream = self.extractor.stream_custom_query(custom_query)

        # 2. Transformer Stream: Chain the raw stream into the transformer
        transformed_data_stream = self.transformer.transform_stream(table_name, raw_data_stream)

        # 3. Loader Stream Consumer: Feed the transformed stream into the loader
        config = ETLConfig.get_config(table_name)
        load_result = await self.loader.load_data_streaming(
            table_name=table_name,
            data_stream=transformed_data_stream,
            primary_key=config.primary_key,
            upsert=True
        )

        if load_result.status in ["success", "partial_success"]:
            return load_result.inserted_records
        else:
            # Raise an exception with the error from the loader
            raise Exception(f"Load failed for {table_name}: {load_result.error_message}")

    @staticmethod
    # En app/etl/coordinators/calendar_driven_coordinator.py
    def _build_campaign_query(
            table_name: str,
            campaign: CampaignWindow,
            start_date: date,
            end_date: date
    ) -> str:
        """
        Builds a BigQuery SQL query with precise, campaign-specific filters.

        IMPROVED: Now filters by both time window AND campaign identifier (`archivo`)
        to prevent data leakage between overlapping campaigns.
        """
        base_query = ETLConfig.get_query_template(table_name)

        # Base filter for the time window - common to most tables
        time_filter = f"BETWEEN '{start_date}' AND '{end_date}'"

        # Specific filters for each table type
        if table_name == 'raw_asignaciones':
            # Asignaciones a menudo usan el `archivo` exacto como identificador
            campaign_filter = f"(archivo = '{campaign.archivo}' OR DATE(creado_el) {time_filter})"

        elif table_name == 'raw_trandeuda':
            # Trandeuda puede estar relacionado por el nombre base del archivo
            # Ejemplo: Cartera_Agencia_..._20250401 y Cartera_Agencia_..._20250401_25
            base_archivo = campaign.archivo.split('_')[0]
            campaign_filter = f"(archivo LIKE '{base_archivo}%' AND DATE(creado_el) {time_filter})"

        elif table_name == 'raw_pagos':
            # Los pagos no tienen `archivo`, asÃ­ que el filtro de tiempo es el principal,
            # pero podrÃ­amos aÃ±adir un filtro por producto si estuviera disponible.
            # Por ahora, el filtro de tiempo es el Ãºnico posible.
            campaign_filter = f"fecha_pago {time_filter}"

        elif table_name == 'gestiones_unificadas':
            # Las gestiones tampoco tienen `archivo`. El filtro de tiempo es clave.
            campaign_filter = f"DATE(timestamp_gestion) {time_filter}"

        else:
            # Fallback por si se aÃ±ade una tabla no reconocida
            campaign_filter = "1=1"

        # Replace the placeholder in the base query template
        return base_query.format(incremental_filter=campaign_filter)

    def cancel(self):
        """Signals the coordinator to stop processing after the current batch."""
        if self._is_running:
            self.logger.warning("ðŸ›‘ Cancellation signal received. Process will stop after the current batch.")
            self._cancel_event.set()

    async def catch_up_all_campaigns(
            self,
            batch_size: int = 5,
            max_campaigns: Optional[int] = None,
            force_refresh: bool = False
    ) -> Dict[str, Any]:
        """Intelligently loads data for all campaigns, with watermark checks and cancellation."""
        if self._is_running:
            self.logger.warning("Catch-up process is already running. Ignoring new request.")
            return {'status': 'already_running', 'message': 'A catch-up process is already in progress.'}

        self._is_running = True
        self._cancel_event.clear()
        start_time = datetime.now()

        try:
            self.logger.info(f"ðŸš€ Starting intelligent catch-up (force_refresh={force_refresh})")
            await self._initialize_components()
            all_campaigns = await self.get_campaign_windows(limit=max_campaigns)

            if not all_campaigns:
                return {'status': 'aborted', 'message': 'No campaigns found', 'duration_seconds': 0}

            # Filter campaigns to be processed
            campaigns_to_process = []
            for campaign in all_campaigns:
                watermark = await self.watermark_manager.get_watermark(self.get_watermark_name(campaign.archivo))
                is_closed_and_done = (
                            campaign.estado_cartera != 'ABIERTA' and watermark and watermark.last_extraction_status == 'success')
                if force_refresh or not is_closed_and_done:
                    campaigns_to_process.append(campaign)

            self.logger.info(f"Found {len(all_campaigns)} total campaigns. "
                             f"Skipping {len(all_campaigns) - len(campaigns_to_process)} already processed. "
                             f"Processing {len(campaigns_to_process)}.")

            if not campaigns_to_process:
                return {'status': 'success', 'message': 'All campaigns are up-to-date.'}

            results = []
            total_to_process = len(campaigns_to_process)
            for i in range(0, total_to_process, batch_size):
                if self._cancel_event.is_set():
                    self.logger.info("Process cancelled by user. Stopping batch processing.")
                    break

                batch = campaigns_to_process[i:i + batch_size]
                self.logger.info(f"ðŸ“¦ Processing batch {i // batch_size + 1}: {len(batch)} campaigns...")

                tasks = [self.load_campaign_data(c) for c in batch]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                for res in batch_results:
                    if isinstance(res, Exception):
                        self.logger.error(f"Task failed in batch: {res}", exc_info=True)
                    else:
                        results.append(res)

            # --- Summary Calculation ---
            _duration = (datetime.now() - start_time).total_seconds()
            summary = {
                'status': 'cancelled' if self._cancel_event.is_set() else 'completed',
                'force_refresh': force_refresh,
                'campaigns_total': len(all_campaigns),
                'campaigns_processed': len(results),
                'campaigns_successful': sum(1 for r in results if r.status == 'success'),
                'campaigns_partial': sum(1 for r in results if r.status == 'partial_success'),
                'campaigns_failed': sum(1 for r in results if r.status == 'failed'),
                # ... (resto del resumen)
            }
            return summary

        finally:
            self._is_running = False
            self._cancel_event.clear()


# --- Singleton Instance ---
_coordinator: Optional[CalendarDrivenCoordinator] = None


def get_calendar_coordinator() -> CalendarDrivenCoordinator:
    """Get the singleton calendar coordinator instance."""
    global _coordinator
    if _coordinator is None:
        _coordinator = CalendarDrivenCoordinator()
    return _coordinator
-- ./app/etl/watermarks.py --
"""
ðŸŽ¯ Watermark System for Incremental Extractions - asyncpg Pure
Production-ready tracking of extraction state without SQLAlchemy overhead

Manages watermarks (last extraction timestamps) for each table,
ensuring reliable incremental processing and recovery from failures.
"""

from datetime import datetime, timezone
from typing import Optional, Dict, Any, List
import json
import logging
from dataclasses import dataclass

from app.database.connection import get_database_manager, execute_query, DatabaseManager


@dataclass
class WatermarkInfo:
    """Information about a table's extraction watermark"""
    table_name: str
    last_extracted_at: datetime
    last_extraction_status: str  # 'success', 'failed', 'running'
    records_extracted: int
    extraction_duration_seconds: float
    error_message: Optional[str] = None
    extraction_id: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None


class WatermarkManager:
    """
    Manages extraction watermarks for incremental ETL - asyncpg Pure
    
    Features:
    - Pure asyncpg for maximum performance
    - Atomic watermark updates
    - Recovery from failed extractions
    - Extraction metadata tracking
    """
    
    def __init__(self, db_manager: Optional[DatabaseManager] = None):
        self.db_manager = db_manager
        self.logger = logging.getLogger(__name__)
    
    async def get_db(self) -> DatabaseManager:
        """Get database manager instance"""
        if self.db_manager is None:
            self.db_manager = await get_database_manager()
        return self.db_manager
    
    async def ensure_watermark_table(self) -> None:
        """Create watermark table if it doesn't exist"""
        # Check if table exists first
        db = await self.get_db()
        table_exists = await db.table_exists("etl_watermarks")
        
        if table_exists:
            self.logger.info("âœ… Watermark table already exists")
            return
        
        # Create table with all constraints and indexes
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS etl_watermarks (
            id SERIAL PRIMARY KEY,
            table_name VARCHAR(100) NOT NULL UNIQUE,
            last_extracted_at TIMESTAMP WITH TIME ZONE NOT NULL,
            last_extraction_status VARCHAR(20) NOT NULL DEFAULT 'success',
            records_extracted INTEGER DEFAULT 0,
            extraction_duration_seconds FLOAT DEFAULT 0.0,
            error_message TEXT,
            extraction_id VARCHAR(50),
            metadata JSONB DEFAULT '{}',
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
        );
        
        CREATE INDEX IF NOT EXISTS idx_etl_watermarks_table_name 
            ON etl_watermarks(table_name);
        CREATE INDEX IF NOT EXISTS idx_etl_watermarks_status 
            ON etl_watermarks(last_extraction_status);
        CREATE INDEX IF NOT EXISTS idx_etl_watermarks_updated 
            ON etl_watermarks(updated_at);
        """
        
        await execute_query(create_table_sql)
        self.logger.info("âœ… Watermark table created with indexes")
    
    async def get_watermark(self, table_name: str) -> Optional[WatermarkInfo]:
        """Get current watermark for a table"""
        query = """
        SELECT 
            table_name,
            last_extracted_at,
            last_extraction_status,
            records_extracted,
            extraction_duration_seconds,
            error_message,
            extraction_id,
            created_at,
            updated_at
        FROM etl_watermarks 
        WHERE table_name = $1
        """
        
        row = await execute_query(query, table_name, fetch="one")
        
        if row:
            return WatermarkInfo(**dict(row))
        return None
    
    async def get_last_extraction_time(self, table_name: str) -> Optional[datetime]:
        """Get just the last extraction timestamp for a table"""
        watermark = await self.get_watermark(table_name)
        return watermark.last_extracted_at if watermark else None
    
    async def start_extraction(self, table_name: str, extraction_id: str) -> None:
        """Mark extraction as started (for monitoring)"""
        upsert_sql = """
        INSERT INTO etl_watermarks (
            table_name, 
            last_extracted_at, 
            last_extraction_status,
            extraction_id,
            updated_at
        ) VALUES (
            $1, $2, 'running', $3, CURRENT_TIMESTAMP
        )
        ON CONFLICT (table_name) 
        DO UPDATE SET 
            last_extraction_status = 'running',
            extraction_id = $3,
            updated_at = CURRENT_TIMESTAMP
        """
        
        await execute_query(
            upsert_sql, 
            table_name, 
            datetime.now(timezone.utc), 
            extraction_id
        )
        
        self.logger.info(f"ðŸ Started extraction for {table_name} (ID: {extraction_id})")
    
    async def update_watermark(
        self, 
        table_name: str, 
        timestamp: datetime,
        records_extracted: int = 0,
        extraction_duration_seconds: float = 0.0,
        status: str = "success",
        error_message: Optional[str] = None,
        extraction_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> None:
        """Update watermark for successful or failed extraction"""
        
        upsert_sql = """
        INSERT INTO etl_watermarks (
            table_name, 
            last_extracted_at, 
            last_extraction_status,
            records_extracted,
            extraction_duration_seconds,
            error_message,
            extraction_id,
            metadata,
            created_at,
            updated_at
        ) VALUES (
            $1, $2, $3, $4, $5, $6, $7, $8, 
            CURRENT_TIMESTAMP, CURRENT_TIMESTAMP
        )
        ON CONFLICT (table_name) 
        DO UPDATE SET 
            last_extracted_at = $2,
            last_extraction_status = $3,
            records_extracted = $4,
            extraction_duration_seconds = $5,
            error_message = $6,
            extraction_id = $7,
            metadata = COALESCE($8, etl_watermarks.metadata),
            updated_at = CURRENT_TIMESTAMP
        """
        
        metadata_json = json.dumps(metadata) if metadata else None
        
        await execute_query(
            upsert_sql,
            table_name,
            timestamp,
            status,
            records_extracted,
            extraction_duration_seconds,
            error_message,
            extraction_id,
            metadata_json
        )
        
        status_emoji = "âœ…" if status == "success" else "âŒ" if status == "failed" else "ðŸ”„"
        self.logger.info(
            f"{status_emoji} Updated watermark for {table_name}: {timestamp} "
            f"({records_extracted} records, {status})"
        )
    
    async def get_failed_extractions(self) -> List[WatermarkInfo]:
        """Get all tables with failed last extraction"""
        query = """
        SELECT 
            table_name,
            last_extracted_at,
            last_extraction_status,
            records_extracted,
            extraction_duration_seconds,
            error_message,
            extraction_id,
            created_at,
            updated_at
        FROM etl_watermarks 
        WHERE last_extraction_status = 'failed'
        ORDER BY updated_at DESC
        """
        
        rows = await execute_query(query, fetch="all")
        return [WatermarkInfo(**dict(row)) for row in rows]
    
    async def get_running_extractions(self) -> List[WatermarkInfo]:
        """Get all extractions currently marked as running"""
        query = """
        SELECT 
            table_name,
            last_extracted_at,
            last_extraction_status,
            records_extracted,
            extraction_duration_seconds,
            error_message,
            extraction_id,
            created_at,
            updated_at
        FROM etl_watermarks 
        WHERE last_extraction_status = 'running'
        ORDER BY updated_at DESC
        """
        
        rows = await execute_query(query, fetch="all")
        return [WatermarkInfo(**dict(row)) for row in rows]
    
    async def cleanup_stale_extractions(self, timeout_minutes: int = 30) -> int:
        """
        Clean up extractions that have been running too long
        
        Returns number of stale extractions cleaned up
        """
        cleanup_sql = """
        UPDATE etl_watermarks 
        SET 
            last_extraction_status = 'failed',
            error_message = 'Extraction timed out - marked as failed by cleanup',
            updated_at = CURRENT_TIMESTAMP
        WHERE 
            last_extraction_status = 'running'
            AND updated_at < CURRENT_TIMESTAMP - INTERVAL '%s minutes'
        RETURNING table_name
        """
        
        cleaned_tables = await execute_query(
            cleanup_sql % timeout_minutes, 
            fetch="all"
        )
        
        cleaned_count = len(cleaned_tables)
        
        if cleaned_count > 0:
            table_names = [row['table_name'] for row in cleaned_tables]
            self.logger.warning(
                f"ðŸ§¹ Cleaned up {cleaned_count} stale extractions: {table_names}"
            )
        
        return cleaned_count
    
    async def reset_watermark(self, table_name: str, timestamp: datetime) -> None:
        """Reset watermark to a specific timestamp (for manual recovery)"""
        await self.update_watermark(
            table_name=table_name,
            timestamp=timestamp,
            status="reset",
            records_extracted=0,
            error_message=f"Manually reset to {timestamp}"
        )
        
        self.logger.warning(f"ðŸ”„ Reset watermark for {table_name} to {timestamp}")
    
    async def delete_watermark(self, table_name: str) -> None:
        """Delete watermark for a table (careful!)"""
        delete_sql = "DELETE FROM etl_watermarks WHERE table_name = $1"
        
        result = await execute_query(delete_sql, table_name)
        
        # Extract row count from result string "DELETE n"
        row_count = int(result.split()[-1]) if result.startswith("DELETE") else 0
        
        if row_count > 0:
            self.logger.warning(f"ðŸ—‘ï¸ Deleted watermark for {table_name}")
        else:
            self.logger.info(f"â„¹ï¸ No watermark found for {table_name}")
    
    async def get_all_watermarks(self) -> List[WatermarkInfo]:
        """Get all watermarks for monitoring dashboard"""
        query = """
        SELECT 
            table_name,
            last_extracted_at,
            last_extraction_status,
            records_extracted,
            extraction_duration_seconds,
            error_message,
            extraction_id,
            created_at,
            updated_at
        FROM etl_watermarks 
        ORDER BY table_name
        """
        
        rows = await execute_query(query, fetch="all")
        return [WatermarkInfo(**dict(row)) for row in rows]
    
    async def get_extraction_summary(self) -> Dict[str, Any]:
        """Get summary statistics for monitoring"""
        summary_sql = """
        SELECT 
            COUNT(*) as total_tables,
            COUNT(*) FILTER (WHERE last_extraction_status = 'success') as successful_tables,
            COUNT(*) FILTER (WHERE last_extraction_status = 'failed') as failed_tables,
            COUNT(*) FILTER (WHERE last_extraction_status = 'running') as running_tables,
            COALESCE(SUM(records_extracted), 0) as total_records_extracted,
            COALESCE(AVG(extraction_duration_seconds), 0) as avg_extraction_time,
            MAX(updated_at) as last_activity
        FROM etl_watermarks
        """
        
        row = await execute_query(summary_sql, fetch="one")
        
        if row:
            summary = dict(row)
            # Convert to proper types for JSON serialization
            summary['avg_extraction_time'] = float(summary['avg_extraction_time'])
            summary['total_records_extracted'] = int(summary['total_records_extracted'])
            return summary
        
        return {
            "total_tables": 0,
            "successful_tables": 0, 
            "failed_tables": 0,
            "running_tables": 0,
            "total_records_extracted": 0,
            "avg_extraction_time": 0.0,
            "last_activity": None
        }


# ðŸŽ¯ Global watermark manager instance
_watermark_manager: Optional[WatermarkManager] = None


async def get_watermark_manager() -> WatermarkManager:
    """Get singleton watermark manager instance"""
    global _watermark_manager
    
    if _watermark_manager is None:
        _watermark_manager = WatermarkManager()
        await _watermark_manager.ensure_watermark_table()
    
    return _watermark_manager


# ðŸš€ Convenience functions for common operations
async def get_last_extracted(table_name: str) -> Optional[datetime]:
    """Quick access to last extraction time"""
    manager = await get_watermark_manager()
    return await manager.get_last_extraction_time(table_name)


async def mark_extraction_success(
    table_name: str, 
    records_count: int, 
    duration_seconds: float,
    extraction_id: Optional[str] = None
) -> None:
    """Mark extraction as successful"""
    manager = await get_watermark_manager()
    await manager.update_watermark(
        table_name=table_name,
        timestamp=datetime.now(timezone.utc),
        records_extracted=records_count,
        extraction_duration_seconds=duration_seconds,
        status="success",
        extraction_id=extraction_id
    )


async def mark_extraction_failed(
    table_name: str, 
    error_message: str,
    extraction_id: Optional[str] = None
) -> None:
    """Mark extraction as failed"""
    manager = await get_watermark_manager()
    await manager.update_watermark(
        table_name=table_name,
        timestamp=datetime.now(timezone.utc),
        status="failed",
        error_message=error_message,
        extraction_id=extraction_id
    )


async def watermark_health_check() -> Dict[str, Any]:
    """Health check for watermark system"""
    try:
        manager = await get_watermark_manager()
        summary = await manager.get_extraction_summary()
        
        return {
            "status": "healthy",
            "watermark_system": "operational",
            "total_tables_tracked": summary["total_tables"],
            "last_activity": summary["last_activity"].isoformat() if summary["last_activity"] else None
        }
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "watermark_system": "failed"
        }

-- ./app/etl/loaders/postgres_loader.py --
"""
ðŸŽ¯ High-Performance PostgreSQL Loader with pure asyncpg - STREAMING FIX
Efficient, asynchronous data loading for ETL pipelines.

Features:
- Pure asyncpg for maximum performance and low overhead
- Dynamic UPSERT statements with `ON CONFLICT DO UPDATE`
- Asynchronous streaming and batch processing
- Data validation and sanitization
- Detailed load statistics and error reporting

CRITICAL STREAMING FIX: Replaced individual record processing (23k+ queries) 
with true batch processing using executemany() for efficient streaming loads
"""

import time
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass
import logging

from app.database.connection import get_database_manager, DatabaseManager
from app.core.logging import LoggerMixin

logger = logging.getLogger(__name__)


@dataclass
class LoadResult:
    """Result information from a load operation"""
    table_name: str
    total_records: int
    inserted_records: int
    updated_records: int
    skipped_records: int
    load_duration_seconds: float
    status: str
    error_message: Optional[str] = None


class PostgresLoader(LoggerMixin):
    """
    A high-performance data loader for PostgreSQL using asyncpg.
    STREAMING FIX: True batch processing for efficient streaming pipeline
    """

    def __init__(self, db_manager: Optional[DatabaseManager] = None):
        super().__init__()
        self.db_manager = db_manager
        self.max_batch_size = 1000
        self.connection_timeout = 30

    async def _get_db_manager(self) -> DatabaseManager:
        if self.db_manager is None:
            self.db_manager = await get_database_manager()
        return self.db_manager

    def _validate_and_sanitize_batch(
        self,
        data: List[Dict[str, Any]],
        primary_key: List[str]
    ) -> List[Dict[str, Any]]:
        """
        Validates and sanitizes a batch of data before loading.
        FIXED: Removed automatic fecha_procesamiento addition that caused errors
        """
        if not data:
            return []

        validated_data = []
        for i, record in enumerate(data):
            # Check for null primary key values
            if any(record.get(pk) is None for pk in primary_key):
                self.logger.warning(f"Record {i} has a null primary key value. Skipping.")
                continue

            # Sanitize and format data - FIXED: Only sanitize existing data
            sanitized_record = {}
            for key, value in record.items():
                if isinstance(value, str) and 'T' in value and ('Z'in value or '+' in value):
                    try:
                        sanitized_record[key] = datetime.fromisoformat(value.replace('Z', '+00:00'))
                    except ValueError:
                        sanitized_record[key] = value
                else:
                    sanitized_record[key] = value

            validated_data.append(sanitized_record)

        return validated_data

    async def load_data_batch(
        self,
        table_name: str,
        data: List[Dict[str, Any]],
        primary_key: List[str],
        upsert: bool = True,
        validate: bool = True
    ) -> LoadResult:
        """
        ðŸš€ STREAMING FIX: Loads a batch using TRUE batch processing with executemany()
        
        BEFORE: 23,333 individual queries causing timeout/hanging
        AFTER: Single executemany() operation for maximum streaming efficiency
        """
        start_time = time.time()

        if not data:
            return LoadResult(
                table_name=table_name,
                total_records=0,
                inserted_records=0,
                updated_records=0,
                skipped_records=0,
                load_duration_seconds=0.0,
                status="success"
            )

        if validate:
            validated_data = self._validate_and_sanitize_batch(data, primary_key)
            skipped_count = len(data) - len(validated_data)
            data = validated_data
        else:
            skipped_count = 0

        if not data:
            return LoadResult(
                table_name=table_name,
                total_records=skipped_count,
                inserted_records=0,
                updated_records=0,
                skipped_records=skipped_count,
                load_duration_seconds=time.time() - start_time,
                status="success",
                error_message="No valid records to load after validation."
            )

        db = await self._get_db_manager()
        pool = await db.get_pool()

        async with pool.acquire() as conn:
            try:
                # ðŸš€ STREAMING FIX: Build query once for batch processing
                columns = list(data[0].keys())
                columns_str = ", ".join(f'"{c}"' for c in columns)
                placeholders = ", ".join(f"${i+1}" for i in range(len(columns)))
                pk_str = ", ".join(f'"{pk}"' for pk in primary_key)
                
                update_columns = [col for col in columns if col not in primary_key]
                update_str = ", ".join(f'"{col}" = EXCLUDED."{col}"' for col in update_columns)

                if upsert and update_columns:
                    query = f"""
                        INSERT INTO {table_name} ({columns_str})
                        VALUES ({placeholders})
                        ON CONFLICT ({pk_str}) DO UPDATE SET {update_str}
                    """
                else:
                    query = f"""
                        INSERT INTO {table_name} ({columns_str})
                        VALUES ({placeholders})
                    """

                # ðŸš€ STREAMING FIX: Prepare all values for executemany()
                batch_values = []
                for record in data:
                    values = [record.get(col) for col in columns]
                    batch_values.append(values)

                # ðŸš€ STREAMING FIX: Single executemany() call instead of 23k individual queries
                self.logger.debug(f"Executing batch INSERT for {len(batch_values)} records into {table_name}")
                
                await conn.executemany(query, batch_values)
                
                inserted_count = len(batch_values)
                duration = time.time() - start_time
                
                self.logger.info(f"âœ… Loaded {inserted_count} records for {table_name}")

                return LoadResult(
                    table_name=table_name,
                    total_records=len(data) + skipped_count,
                    inserted_records=inserted_count,
                    updated_records=0,  # Simplified for performance
                    skipped_records=skipped_count,
                    load_duration_seconds=duration,
                    status="success"
                )

            except Exception as e:
                duration = time.time() - start_time
                error_msg = f"Failed to load batch into {table_name}: {e}"
                self.logger.error(error_msg)
                self.logger.debug(f"Failed batch size: {len(data)} records")
                
                return LoadResult(
                    table_name=table_name,
                    total_records=len(data) + skipped_count,
                    inserted_records=0,
                    updated_records=0,
                    skipped_records=len(data) + skipped_count,
                    load_duration_seconds=duration,
                    status="failed",
                    error_message=error_msg
                )

    async def load_data_streaming(
        self,
        table_name: str,
        data_stream: AsyncGenerator[List[Dict[str, Any]], None],
        primary_key: List[str],
        upsert: bool = True
    ) -> LoadResult:
        """
        ðŸš€ STREAMING FIX: Loads data from an async stream in efficient batches
        
        This is the method called by CalendarDrivenCoordinator._load_campaign_table()
        """
        start_time = time.time()
        total_records, total_inserted, total_skipped = 0, 0, 0
        errors = []

        self.logger.debug(f"Starting streaming load for {table_name}")

        try:
            async for batch in data_stream:
                if not batch:
                    continue

                # ðŸš€ STREAMING FIX: Each batch now uses efficient executemany()
                batch_result = await self.load_data_batch(
                    table_name,
                    batch,
                    primary_key,
                    upsert=upsert
                )

                total_records += batch_result.total_records
                total_inserted += batch_result.inserted_records
                total_skipped += batch_result.skipped_records

                if batch_result.status == "failed":
                    errors.append(batch_result.error_message)
                    self.logger.warning(f"Batch failed for {table_name}: {batch_result.error_message}")

            duration = time.time() - start_time
            status = "success" if not errors else ("partial_success" if total_inserted > 0 else "failed")
            error_message = "; ".join(errors) if errors else None

            # ðŸŽ¯ This is what gets returned to _load_campaign_table()
            final_result = LoadResult(
                table_name=table_name,
                total_records=total_records,
                inserted_records=total_inserted,
                updated_records=0,
                skipped_records=total_skipped,
                load_duration_seconds=duration,
                status=status,
                error_message=error_message
            )

            if status == "success":
                self.logger.info(f"ðŸŽ¯ Streaming load completed for {table_name}: {total_inserted} records loaded")
            else:
                self.logger.error(f"âŒ Streaming load failed for {table_name}: {error_message}")

            return final_result

        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"Streaming load failed for {table_name}: {e}"
            self.logger.error(error_msg, exc_info=True)
            
            return LoadResult(
                table_name=table_name,
                total_records=total_records,
                inserted_records=total_inserted,
                updated_records=0,
                skipped_records=total_records - total_inserted,
                load_duration_seconds=duration,
                status="failed",
                error_message=error_msg
            )

    async def truncate_and_load(
        self,
        table_name: str,
        data: List[Dict[str, Any]],
        primary_key: List[str]
    ) -> LoadResult:
        """
        Truncates a table and then loads new data.
        """
        start_time = time.time()
        db = await self._get_db_manager()

        try:
            await db.execute_query(f"TRUNCATE TABLE {table_name} RESTART IDENTITY")
            self.logger.info(f"Truncated table: {table_name}")

            load_result = await self.load_data_batch(
                table_name,
                data,
                primary_key,
                upsert=False
            )

            load_result.load_duration_seconds = time.time() - start_time
            return load_result

        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"Failed truncate and load for {table_name}: {e}"
            self.logger.error(error_msg)
            return LoadResult(
                table_name=table_name,
                total_records=len(data),
                inserted_records=0,
                updated_records=0,
                skipped_records=len(data),
                load_duration_seconds=duration,
                status="failed",
                error_message=error_msg
            )

async def get_loader() -> PostgresLoader:
    """Returns a configured instance of the PostgresLoader."""
    db_manager = await get_database_manager()
    return PostgresLoader(db_manager)

-- ./app/etl/transformers/unified_transformer.py --
"""
ðŸ”„ Unified Transformer Registry - Complete 3-Layer Pipeline - CIRCULAR IMPORT FIXED
Connects all transformers: Raw â†’ Business Logic â†’ Mart Tables

ARCHITECTURE:
1. RawDataTransformer: BigQuery â†’ Raw Tables (staging)
2. BusinessLogicTransformer: Campaign logic + deduplication  
3. DataTransformer: Final KPIs â†’ Mart Tables

FIXED: Removed circular import completely - all transformers defined locally
"""

from typing import List, Dict, Any, Optional, AsyncGenerator
from app.etl.transformers.raw_data_transformer import get_raw_transformer_registry, RawTransformerRegistry
from app.etl.transformers.business_logic_transformer import get_business_transformer, BusinessLogicTransformer
from app.core.logging import LoggerMixin


# âœ… FIXED: Define mart transformers locally to break circular import
class DataTransformer:
    """
    Basic data transformer for mart tables
    """
    
    def __init__(self):
        self.stats = {
            'records_processed': 0,
            'records_transformed': 0,
            'transformation_errors': 0
        }
    
    def transform_dashboard_data(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Transform data for dashboard_data table"""
        transformed = []
        for record in raw_data:
            transformed_record = {
                **record,
                'processed_at': record.get('extraction_timestamp'),
                'source': 'unified_transformer'
            }
            transformed.append(transformed_record)
        
        self.stats['records_processed'] += len(raw_data)
        self.stats['records_transformed'] += len(transformed)
        return transformed
    
    def transform_evolution_data(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Transform data for evolution_data table"""
        return self.transform_dashboard_data(raw_data)
    
    def transform_assignment_data(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Transform data for assignment_data table"""
        return self.transform_dashboard_data(raw_data)
    
    def transform_operation_data(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Transform data for operation_data table"""
        return self.transform_dashboard_data(raw_data)
    
    def transform_productivity_data(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Transform data for productivity_data table"""
        return self.transform_dashboard_data(raw_data)
    
    def get_transformation_stats(self) -> Dict[str, int]:
        """Get transformation statistics"""
        return self.stats.copy()
    
    def reset_stats(self):
        """Reset transformation statistics"""
        self.stats = {
            'records_processed': 0,
            'records_transformed': 0,
            'transformation_errors': 0
        }


class MartTransformerRegistry:
    """
    Registry for mart table transformers
    """
    
    def __init__(self):
        self.transformer = DataTransformer()
        self.supported_mart_tables = [
            'dashboard_data',
            'evolution_data', 
            'assignment_data',
            'operation_data',
            'productivity_data'
        ]
    
    def transform_table_data(self, table_name: str, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Transform data for a specific mart table"""
        
        if table_name == 'dashboard_data':
            return self.transformer.transform_dashboard_data(raw_data)
        elif table_name == 'evolution_data':
            return self.transformer.transform_evolution_data(raw_data)
        elif table_name == 'assignment_data':
            return self.transformer.transform_assignment_data(raw_data)
        elif table_name == 'operation_data':
            return self.transformer.transform_operation_data(raw_data)
        elif table_name == 'productivity_data':
            return self.transformer.transform_productivity_data(raw_data)
        else:
            raise ValueError(f"No mart transformer found for table: {table_name}")
    
    def get_supported_tables(self) -> List[str]:
        """Get list of supported mart tables"""
        return self.supported_mart_tables.copy()
    
    def get_transformation_stats(self) -> Dict[str, Any]:
        """Get transformation statistics"""
        return {
            'mart_stats': self.transformer.get_transformation_stats(),
            'supported_tables': len(self.supported_mart_tables)
        }


class UnifiedTransformerRegistry(LoggerMixin):
    """
    Unified transformer that handles all 3 pipeline layers:
    - Raw staging tables (minimal transformation)
    - Business logic processing (complex deduplication)  
    - Mart tables (final dashboard consumption)
    """
    
    def __init__(self):
        super().__init__()
        
        # Initialize all transformer layers - NO CIRCULAR IMPORTS
        self.raw_transformer: RawTransformerRegistry = get_raw_transformer_registry()
        self.business_transformer: BusinessLogicTransformer = get_business_transformer()
        self.mart_transformer: MartTransformerRegistry = MartTransformerRegistry()  # âœ… Local class
        
        # Combined mapping of all supported tables
        self.all_transformers = {
            # Raw tables (BigQuery â†’ PostgreSQL staging)
            **{table: 'raw' for table in self.raw_transformer.get_supported_raw_tables()},
            
            # Mart tables (Staging â†’ Final dashboard tables)
            **{table: 'mart' for table in self.mart_transformer.get_supported_tables()}
        }
        
        self.logger.info(f"Unified transformer initialized with {len(self.all_transformers)} table mappings")
    
    def transform_table_data(self, table_name: str, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Transform data for any table using the appropriate transformer layer
        
        Args:
            table_name: Name of the target table
            raw_data: Raw data from BigQuery
            
        Returns:
            Transformed data ready for PostgreSQL
        """
        
        if table_name not in self.all_transformers:
            raise ValueError(f"No transformer found for table: {table_name}")
        
        transformer_type = self.all_transformers[table_name]
        
        try:
            if transformer_type == 'raw':
                # Raw table transformation (minimal)
                self.logger.debug(f"Using RawDataTransformer for {table_name}")
                return self.raw_transformer.transform_raw_table_data(table_name, raw_data)
                
            elif transformer_type == 'mart':
                # Mart table transformation (final KPIs)
                self.logger.debug(f"Using DataTransformer for {table_name}")
                return self.mart_transformer.transform_table_data(table_name, raw_data)
                
            else:
                raise ValueError(f"Unknown transformer type: {transformer_type}")
                
        except Exception as e:
            self.logger.error(f"Transformation failed for {table_name}: {str(e)}")
            raise
    
    def get_supported_tables(self) -> List[str]:
        """Get all supported table names across all transformer layers"""
        return list(self.all_transformers.keys())
    
    def get_transformation_stats(self) -> Dict[str, Any]:
        """Get combined transformation statistics from all layers"""
        return {
            'raw_stats': self.raw_transformer.get_transformation_stats(),
            'mart_stats': self.mart_transformer.get_transformation_stats(),
            'business_stats': self.business_transformer.get_processing_stats(),
            'supported_tables': len(self.all_transformers),
            'raw_tables': len([t for t, type in self.all_transformers.items() if type == 'raw']),
            'mart_tables': len([t for t, type in self.all_transformers.items() if type == 'mart'])
        }
    
    def reset_all_stats(self):
        """Reset statistics across all transformer layers"""
        self.raw_transformer.transformer.reset_stats()
        self.mart_transformer.transformer.reset_stats()
        self.business_transformer.reset_stats()
    
    def get_table_info(self, table_name: str) -> Dict[str, Any]:
        """Get information about a specific table and its transformer"""
        if table_name not in self.all_transformers:
            return {"error": f"Table {table_name} not supported"}
        
        transformer_type = self.all_transformers[table_name]
        
        return {
            "table_name": table_name,
            "transformer_type": transformer_type,
            "transformer_layer": {
                'raw': 'BigQuery â†’ Raw Tables (staging)',
                'mart': 'Raw/Auxiliary â†’ Mart Tables (dashboard)'
            }.get(transformer_type, 'unknown'),
            "supported": True
        }
    
    async def process_campaign_business_logic(self, archivo: str) -> Dict[str, Any]:
        """
        Process complete business logic for a campaign
        
        This method orchestrates the complex business logic transformation:
        1. Build cuenta_campana_state
        2. Map gestiones to accounts  
        3. Deduplicate payments
        4. Calculate accurate KPIs
        
        Args:
            archivo: Campaign identifier
            
        Returns:
            Processing result with statistics
        """
        self.logger.info(f"ðŸŽ¯ Starting business logic processing for campaign: {archivo}")
        
        try:
            # Process complete campaign window with business logic
            result = await self.business_transformer.process_campaign_window(archivo)
            
            # Calculate final KPIs using auxiliary tables
            kpis = await self.business_transformer.calculate_campaign_kpis(archivo)
            
            # Combine results
            result['calculated_kpis'] = kpis
            result['processing_stats'] = self.business_transformer.get_processing_stats()
            
            self.logger.info(f"âœ… Business logic completed for {archivo}: {result}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Business logic failed for {archivo}: {str(e)}")
            raise
    
    def supports_business_logic(self) -> bool:
        """Check if business logic transformer is available"""
        return self.business_transformer is not None
    
    def get_business_logic_status(self) -> Dict[str, Any]:
        """Get status of business logic transformer"""
        return {
            'available': self.supports_business_logic(),
            'stats': self.business_transformer.get_processing_stats() if self.supports_business_logic() else {},
            'description': 'Campaign window deduplication and KPI calculation'
        }

        # En: app/etl/transformers/unified_transformer.py
        # Dentro de la clase UnifiedTransformerRegistry

    async def transform_stream(
            self,
            table_name: str,
            data_stream: AsyncGenerator[List[Dict[str, Any]], None]
    ) -> AsyncGenerator[List[Dict[str, Any]], None]:
        """
        Transforms a stream of data batches.

        Takes an async generator of raw data batches and yields
        an async generator of transformed data batches. This is the
        memory-efficient way to handle transformations.

        Args:
            table_name: The name of the target table.
            data_stream: An async generator yielding batches of raw data.

        Yields:
            A batch of transformed data.
        """
        self.logger.debug(f"Initiating stream transformation for {table_name}...")
        self.reset_all_stats()  # Reset stats before starting a new stream

        async for raw_batch in data_stream:
            if not raw_batch:
                continue

            # Reuse the existing batch transformation logic
            # This is efficient as it operates on one batch at a time
            transformed_batch = self.transform_table_data(table_name, raw_batch)

            if transformed_batch:
                yield transformed_batch


# ðŸŽ¯ Global unified transformer instance
_unified_transformer: Optional[UnifiedTransformerRegistry] = None

def get_unified_transformer_registry() -> UnifiedTransformerRegistry:
    """Get singleton unified transformer registry instance - NO CIRCULAR IMPORTS"""
    global _unified_transformer
    
    if _unified_transformer is None:
        # âœ… FIXED: No import from data_transformer - create directly
        _unified_transformer = UnifiedTransformerRegistry()
    
    return _unified_transformer


# ðŸ”„ Update the main transformer registry to use unified version
def get_transformer_registry() -> UnifiedTransformerRegistry:
    """
    Get transformer registry - now returns unified version
    
    COMPATIBILITY: This maintains the same interface as before
    but now supports all table types (raw + mart)
    """
    return get_unified_transformer_registry()


# ðŸš€ Convenience functions for specific transformer layers
def transform_raw_data(table_name: str, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Transform raw BigQuery data for staging tables"""
    registry = get_unified_transformer_registry()
    return registry.transform_table_data(table_name, raw_data)

def transform_mart_data(table_name: str, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Transform data for final mart tables"""
    registry = get_unified_transformer_registry()
    return registry.transform_table_data(table_name, raw_data)

async def process_campaign_window(archivo: str) -> Dict[str, Any]:
    """Process complete business logic for a campaign window"""
    registry = get_unified_transformer_registry()
    return await registry.process_campaign_business_logic(archivo)

-- ./app/etl/transformers/business_logic_transformer.py --
"""
ðŸŽ¯ Business Logic Transformer - Campaign Window Deduplication
Implements the complex business logic for campaign window processing:

1. Build cuenta_campana_state (universe of manageable accounts per campaign)
2. Map gestiones to accounts (gestion_cuenta_impact) 
3. Deduplicate payments (pago_deduplication)
4. Calculate accurate KPIs for dashboard_data

This transformer handles the core business problem of deduplication and attribution.
"""

import logging
from datetime import datetime, date, timezone
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass
import pandas as pd

from app.etl.extractors.bigquery_extractor import BigQueryExtractor
from app.database.connection import execute_query


@dataclass
class CampaignWindow:
    """Represents a campaign time window with its metadata"""
    archivo: str
    fecha_apertura: date
    fecha_cierre: Optional[date]
    tipo_cartera: str
    anno_asignacion: int
    estado_cartera: str


class BusinessLogicTransformer:
    """
    Implements complex business logic for campaign window processing
    
    Key responsibilities:
    1. Build clean campaign windows from calendario
    2. Map cod_luna -> active accounts per campaign  
    3. Attribute gestiones to correct accounts
    4. Deduplicate payments reliably
    """
    
    def __init__(self, extractor: Optional[BigQueryExtractor] = None):
        self.extractor = extractor or BigQueryExtractor()
        self.logger = logging.getLogger(__name__)
        
        # Processing statistics
        self.stats = {
            'campaigns_processed': 0,
            'accounts_mapped': 0,
            'gestiones_attributed': 0,
            'payments_deduplicated': 0,
            'errors': 0
        }
    
    async def process_campaign_window(self, archivo: str) -> Dict[str, Any]:
        """
        Complete processing for a single campaign window
        
        Args:
            archivo: Campaign identifier
            
        Returns:
            Processing summary with statistics
        """
        try:
            self.logger.info(f"ðŸ Starting campaign window processing: {archivo}")
            
            # Step 1: Get campaign metadata
            campaign = await self._get_campaign_window(archivo)
            if not campaign:
                raise ValueError(f"Campaign not found: {archivo}")
            
            # Step 2: Build account states for this campaign
            accounts_processed = await self._build_cuenta_campana_state(campaign)
            
            # Step 3: Map gestiones to active accounts
            gestiones_processed = await self._map_gestiones_to_accounts(campaign)
            
            # Step 4: Deduplicate payments for this campaign
            payments_processed = await self._deduplicate_campaign_payments(campaign)
            
            # Update statistics
            self.stats['campaigns_processed'] += 1
            self.stats['accounts_mapped'] += accounts_processed
            self.stats['gestiones_attributed'] += gestiones_processed
            self.stats['payments_deduplicated'] += payments_processed
            
            self.logger.info(
                f"âœ… Completed campaign {archivo}: "
                f"{accounts_processed} accounts, {gestiones_processed} gestiones, "
                f"{payments_processed} payments"
            )
            
            return {
                'archivo': archivo,
                'status': 'success',
                'accounts_processed': accounts_processed,
                'gestiones_processed': gestiones_processed,
                'payments_processed': payments_processed
            }
            
        except Exception as e:
            self.stats['errors'] += 1
            self.logger.error(f"âŒ Error processing campaign {archivo}: {str(e)}")
            raise
    
    async def _get_campaign_window(self, archivo: str) -> Optional[CampaignWindow]:
        """Get campaign window definition from calendario"""
        query = """
        SELECT 
            ARCHIVO,
            fecha_apertura,
            fecha_cierre,
            TIPO_CARTERA,
            ANNO_ASIGNACION,
            ESTADO_CARTERA
        FROM raw_calendario_cache
        WHERE ARCHIVO = $1
        """
        
        # Try from cached table first, fallback to BigQuery
        try:
            row = await execute_query(query, archivo, fetch="one")
            if row:
                return CampaignWindow(**dict(row))
        except:
            pass
        
        # Fallback: Extract from BigQuery
        calendario_data = await self.extractor.extract_table_data(
            "raw_calendario", 
            {"archivo": archivo}
        )
        
        if calendario_data:
            row = calendario_data[0]
            return CampaignWindow(
                archivo=row['ARCHIVO'],
                fecha_apertura=row['fecha_apertura'],
                fecha_cierre=row.get('fecha_cierre'),
                tipo_cartera=row['TIPO_CARTERA'],
                anno_asignacion=row['ANNO_ASIGNACION'],
                estado_cartera=row['ESTADO_CARTERA']
            )
        
        return None
    
    async def _build_cuenta_campana_state(self, campaign: CampaignWindow) -> int:
        """
        Build cuenta_campana_state table for this campaign
        
        Logic:
        1. Get all cod_luna assigned to this campaign
        2. For each cod_luna, get all their accounts from asignaciones
        3. Get debt state for each account at campaign start/current
        4. Mark accounts as gestionable based on business rules
        """
        
        # Step 1: Get asignaciones for this campaign
        asignaciones_query = """
        SELECT 
            CAST(cod_luna AS STRING) as cod_luna,
            CAST(cuenta AS STRING) as cuenta,
            archivo,
            fecha_archivo,
            tramo_gestion,
            negocio
        FROM raw_asignaciones_cache
        WHERE archivo = $1
        """
        
        asignaciones = await execute_query(asignaciones_query, campaign.archivo, fetch="all")
        
        if not asignaciones:
            self.logger.warning(f"No asignaciones found for campaign {campaign.archivo}")
            return 0
        
        # Step 2: Process each asignaciÃ³n
        cuenta_states = []
        
        for asignacion in asignaciones:
            cod_luna = asignacion['cod_luna']
            cuenta = asignacion['cuenta']
            
            # Get debt state for this account
            monto_inicial = await self._get_account_debt_at_date(
                cuenta, campaign.fecha_apertura
            )
            
            monto_actual = await self._get_account_debt_at_date(
                cuenta, campaign.fecha_cierre or date.today()
            )
            
            # Business rules for gestionable accounts
            es_cuenta_gestionable = (
                monto_inicial > 0 and  # Had debt at campaign start
                asignacion.get('tramo_gestion', '') != 'EXCLUIDO'  # Not excluded
            )
            
            cuenta_states.append({
                'archivo': campaign.archivo,
                'cod_luna': cod_luna,
                'cuenta': cuenta,
                'fecha_apertura': campaign.fecha_apertura,
                'fecha_cierre': campaign.fecha_cierre,
                'monto_inicial': monto_inicial,
                'monto_actual': monto_actual,
                'fecha_ultima_actualizacion': date.today(),
                'tiene_deuda_activa': monto_actual > 0,
                'es_cuenta_gestionable': es_cuenta_gestionable
            })
        
        # Step 3: Upsert to cuenta_campana_state table
        await self._upsert_cuenta_states(cuenta_states)
        
        return len(cuenta_states)
    
    async def _get_account_debt_at_date(self, cuenta: str, fecha: date) -> float:
        """Get account debt amount at a specific date"""
        
        # Get debt history for this account around the target date
        debt_query = """
        SELECT 
            monto_exigible,
            fecha_archivo,
            ABS(EXTRACT(EPOCH FROM (fecha_archivo - $2::date)) / 86400) as days_diff
        FROM raw_trandeuda_cache
        WHERE cod_cuenta = $1
          AND fecha_archivo <= $2::date
        ORDER BY days_diff ASC
        LIMIT 1
        """
        
        try:
            debt_row = await execute_query(debt_query, cuenta, fecha, fetch="one")
            if debt_row:
                return float(debt_row['monto_exigible'])
        except Exception as e:
            self.logger.warning(f"Could not get debt for account {cuenta} at {fecha}: {e}")
        
        return 0.0
    
    async def _upsert_cuenta_states(self, cuenta_states: List[Dict[str, Any]]) -> None:
        """Bulk upsert cuenta_campana_state records"""
        
        if not cuenta_states:
            return
        
        # Use pandas for efficient bulk operations
        df = pd.DataFrame(cuenta_states)
        
        # Prepare bulk upsert query
        upsert_query = """
        INSERT INTO cuenta_campana_state (
            archivo, cod_luna, cuenta, fecha_apertura, fecha_cierre,
            monto_inicial, monto_actual, fecha_ultima_actualizacion,
            tiene_deuda_activa, es_cuenta_gestionable, updated_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, CURRENT_TIMESTAMP)
        ON CONFLICT (archivo, cod_luna, cuenta)
        DO UPDATE SET
            monto_inicial = EXCLUDED.monto_inicial,
            monto_actual = EXCLUDED.monto_actual,
            fecha_ultima_actualizacion = EXCLUDED.fecha_ultima_actualizacion,
            tiene_deuda_activa = EXCLUDED.tiene_deuda_activa,
            es_cuenta_gestionable = EXCLUDED.es_cuenta_gestionable,
            updated_at = CURRENT_TIMESTAMP
        """
        
        # Execute bulk upsert
        for _, row in df.iterrows():
            await execute_query(
                upsert_query,
                row['archivo'], row['cod_luna'], row['cuenta'],
                row['fecha_apertura'], row['fecha_cierre'],
                row['monto_inicial'], row['monto_actual'],
                row['fecha_ultima_actualizacion'],
                row['tiene_deuda_activa'], row['es_cuenta_gestionable']
            )
    
    async def _map_gestiones_to_accounts(self, campaign: CampaignWindow) -> int:
        """
        Map gestiones to specific accounts that were impacted
        
        Logic:
        1. Get all gestiones for this campaign window
        2. For each gestiÃ³n (cod_luna + timestamp), find active accounts
        3. Create impact records for each account
        """
        
        # Step 1: Get gestiones within campaign window
        gestiones_query = """
        SELECT 
            CAST(cod_luna AS STRING) as cod_luna,
            timestamp_gestion,
            fecha_gestion,
            canal_origen,
            contactabilidad,
            es_contacto_efectivo,
            es_compromiso,
            peso_gestion
        FROM gestiones_unificadas_cache
        WHERE fecha_gestion BETWEEN $1 AND $2
        ORDER BY timestamp_gestion
        """
        
        fecha_fin = campaign.fecha_cierre or date.today()
        gestiones = await execute_query(
            gestiones_query, 
            campaign.fecha_apertura, 
            fecha_fin, 
            fetch="all"
        )
        
        if not gestiones:
            self.logger.info(f"No gestiones found for campaign {campaign.archivo}")
            return 0
        
        # Step 2: For each gestiÃ³n, find impacted accounts
        impact_records = []
        
        for gestion in gestiones:
            cod_luna = gestion['cod_luna']
            
            # Find active accounts for this cod_luna in this campaign
            accounts_query = """
            SELECT cuenta, monto_actual
            FROM cuenta_campana_state
            WHERE archivo = $1 AND cod_luna = $2 AND es_cuenta_gestionable = TRUE
            """
            
            active_accounts = await execute_query(
                accounts_query, 
                campaign.archivo, 
                cod_luna, 
                fetch="all"
            )
            
            # Create impact record for each active account
            for account in active_accounts:
                impact_records.append({
                    'archivo': campaign.archivo,
                    'cod_luna': cod_luna,
                    'timestamp_gestion': gestion['timestamp_gestion'],
                    'cuenta': account['cuenta'],
                    'canal_origen': gestion['canal_origen'],
                    'contactabilidad': gestion['contactabilidad'],
                    'es_contacto_efectivo': gestion['es_contacto_efectivo'],
                    'es_compromiso': gestion['es_compromiso'],
                    'peso_gestion': gestion['peso_gestion'],
                    'monto_deuda_momento': account['monto_actual'],
                    'es_cuenta_con_deuda': account['monto_actual'] > 0
                })
        
        # Step 3: Bulk insert impact records
        await self._insert_gestion_impacts(impact_records)
        
        return len(impact_records)
    
    async def _insert_gestion_impacts(self, impact_records: List[Dict[str, Any]]) -> None:
        """Bulk insert gestion_cuenta_impact records"""
        
        if not impact_records:
            return
        
        insert_query = """
        INSERT INTO gestion_cuenta_impact (
            archivo, cod_luna, timestamp_gestion, cuenta,
            canal_origen, contactabilidad, es_contacto_efectivo,
            es_compromiso, peso_gestion, monto_deuda_momento,
            es_cuenta_con_deuda, created_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, CURRENT_TIMESTAMP)
        ON CONFLICT (archivo, cod_luna, timestamp_gestion, cuenta)
        DO UPDATE SET
            contactabilidad = EXCLUDED.contactabilidad,
            es_contacto_efectivo = EXCLUDED.es_contacto_efectivo,
            es_compromiso = EXCLUDED.es_compromiso,
            monto_deuda_momento = EXCLUDED.monto_deuda_momento,
            updated_at = CURRENT_TIMESTAMP
        """
        
        # Execute bulk insert
        for record in impact_records:
            await execute_query(
                insert_query,
                record['archivo'], record['cod_luna'], record['timestamp_gestion'],
                record['cuenta'], record['canal_origen'], record['contactabilidad'],
                record['es_contacto_efectivo'], record['es_compromiso'],
                record['peso_gestion'], record['monto_deuda_momento'],
                record['es_cuenta_con_deuda']
            )
    
    async def _deduplicate_campaign_payments(self, campaign: CampaignWindow) -> int:
        """
        Deduplicate payments for this campaign window
        
        Logic:
        1. Get all payments for accounts in this campaign
        2. Group by (nro_documento, fecha_pago, monto_cancelado)
        3. Mark first occurrence (by fecha_archivo) as unique
        4. Track all occurrences for audit
        """
        
        # Step 1: Get payments for accounts in this campaign
        payments_query = """
        SELECT DISTINCT
            p.nro_documento,
            p.fecha_pago,
            p.monto_cancelado,
            p.fecha_archivo,
            p.cuenta,
            ccs.cod_luna
        FROM raw_pagos_cache p
        INNER JOIN cuenta_campana_state ccs 
            ON p.cuenta = ccs.cuenta 
            AND ccs.archivo = $1
        WHERE p.fecha_archivo >= $2::date
        ORDER BY p.nro_documento, p.fecha_pago, p.monto_cancelado, p.fecha_archivo
        """
        
        payments = await execute_query(
            payments_query,
            campaign.archivo,
            campaign.fecha_apertura,
            fetch="all"
        )
        
        if not payments:
            self.logger.info(f"No payments found for campaign {campaign.archivo}")
            return 0
        
        # Step 2: Process payments with deduplication logic
        df_payments = pd.DataFrame([dict(p) for p in payments])
        
        # Group by business key to identify duplicates
        payment_groups = df_payments.groupby(['nro_documento', 'fecha_pago', 'monto_cancelado'])
        
        dedup_records = []
        
        for (nro_doc, fecha_pago, monto), group in payment_groups:
            # Sort by fecha_archivo to find first occurrence
            group_sorted = group.sort_values('fecha_archivo')
            first_occurrence = group_sorted.iloc[0]
            
            # Check if payment is within campaign window
            esta_en_ventana = (
                campaign.fecha_apertura <= fecha_pago <= 
                (campaign.fecha_cierre or date.today())
            )
            
            dedup_records.append({
                'archivo': campaign.archivo,
                'cuenta': first_occurrence['cuenta'],
                'nro_documento': nro_doc,
                'fecha_pago': fecha_pago,
                'monto_cancelado': monto,
                'es_pago_unico': True,
                'fecha_primera_carga': group_sorted['fecha_archivo'].min(),
                'fecha_ultima_carga': group_sorted['fecha_archivo'].max(),
                'veces_visto': len(group),
                'esta_en_ventana': esta_en_ventana,
                'cod_luna': first_occurrence['cod_luna'],
                'es_pago_valido': True  # Apply business validation here
            })
        
        # Step 3: Bulk upsert dedup records
        await self._upsert_payment_dedup(dedup_records)
        
        return len(dedup_records)
    
    async def _upsert_payment_dedup(self, dedup_records: List[Dict[str, Any]]) -> None:
        """Bulk upsert pago_deduplication records"""
        
        if not dedup_records:
            return
        
        upsert_query = """
        INSERT INTO pago_deduplication (
            archivo, cuenta, nro_documento, fecha_pago, monto_cancelado,
            es_pago_unico, fecha_primera_carga, fecha_ultima_carga,
            veces_visto, esta_en_ventana, cod_luna, es_pago_valido,
            created_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, CURRENT_TIMESTAMP)
        ON CONFLICT (archivo, nro_documento, fecha_pago, monto_cancelado)
        DO UPDATE SET
            fecha_ultima_carga = EXCLUDED.fecha_ultima_carga,
            veces_visto = EXCLUDED.veces_visto,
            updated_at = CURRENT_TIMESTAMP
        """
        
        # Execute bulk upsert
        for record in dedup_records:
            await execute_query(
                upsert_query,
                record['archivo'], record['cuenta'], record['nro_documento'],
                record['fecha_pago'], record['monto_cancelado'],
                record['es_pago_unico'], record['fecha_primera_carga'],
                record['fecha_ultima_carga'], record['veces_visto'],
                record['esta_en_ventana'], record['cod_luna'],
                record['es_pago_valido']
            )
    
    async def calculate_campaign_kpis(self, archivo: str) -> Dict[str, float]:
        """
        Calculate accurate KPIs for a campaign using auxiliary tables
        
        Returns:
            Dictionary with calculated KPI values
        """
        
        # PCT_COBER: % of gestionable accounts that were contacted
        cober_query = """
        SELECT 
            COUNT(DISTINCT ccs.cuenta) as cuentas_gestionables,
            COUNT(DISTINCT gci.cuenta) as cuentas_contactadas
        FROM cuenta_campana_state ccs
        LEFT JOIN gestion_cuenta_impact gci 
            ON ccs.archivo = gci.archivo 
            AND ccs.cuenta = gci.cuenta
            AND gci.es_contacto_efectivo = TRUE
        WHERE ccs.archivo = $1 AND ccs.es_cuenta_gestionable = TRUE
        """
        
        cober_result = await execute_query(cober_query, archivo, fetch="one")
        
        pct_cober = 0.0
        if cober_result and cober_result['cuentas_gestionables'] > 0:
            pct_cober = (cober_result['cuentas_contactadas'] / 
                        cober_result['cuentas_gestionables']) * 100
        
        # PCT_CONTAC: % of gestiones that were effective contact
        contac_query = """
        SELECT 
            COUNT(*) as total_gestiones,
            COUNT(*) FILTER (WHERE es_contacto_efectivo = TRUE) as contactos_efectivos
        FROM gestion_cuenta_impact
        WHERE archivo = $1
        """
        
        contac_result = await execute_query(contac_query, archivo, fetch="one")
        
        pct_contac = 0.0
        if contac_result and contac_result['total_gestiones'] > 0:
            pct_contac = (contac_result['contactos_efectivos'] / 
                         contac_result['total_gestiones']) * 100
        
        # PCT_EFECTIVIDAD: % of effective contacts that generated commitment
        efect_query = """
        SELECT 
            COUNT(*) FILTER (WHERE es_contacto_efectivo = TRUE) as contactos_efectivos,
            COUNT(*) FILTER (WHERE es_contacto_efectivo = TRUE AND es_compromiso = TRUE) as compromisos
        FROM gestion_cuenta_impact
        WHERE archivo = $1
        """
        
        efect_result = await execute_query(efect_query, archivo, fetch="one")
        
        pct_efectividad = 0.0
        if efect_result and efect_result['contactos_efectivos'] > 0:
            pct_efectividad = (efect_result['compromisos'] / 
                              efect_result['contactos_efectivos']) * 100
        
        # RECUPERO: Total unique payments within campaign window
        recupero_query = """
        SELECT COALESCE(SUM(monto_cancelado), 0) as total_recupero
        FROM pago_deduplication
        WHERE archivo = $1 
          AND es_pago_unico = TRUE 
          AND es_pago_valido = TRUE 
          AND esta_en_ventana = TRUE
        """
        
        recupero_result = await execute_query(recupero_query, archivo, fetch="one")
        recupero = float(recupero_result['total_recupero']) if recupero_result else 0.0
        
        return {
            'pct_cober': round(pct_cober, 2),
            'pct_contac': round(pct_contac, 2),
            'pct_efectividad': round(pct_efectividad, 2),
            'recupero': round(recupero, 2)
        }
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """Get processing statistics"""
        return self.stats.copy()
    
    def reset_stats(self) -> None:
        """Reset processing statistics"""
        self.stats = {
            'campaigns_processed': 0,
            'accounts_mapped': 0,
            'gestiones_attributed': 0,
            'payments_deduplicated': 0,
            'errors': 0
        }


# Global instance for reuse
_business_transformer: Optional[BusinessLogicTransformer] = None

def get_business_transformer() -> BusinessLogicTransformer:
    """Get singleton business transformer instance"""
    global _business_transformer
    
    if _business_transformer is None:
        _business_transformer = BusinessLogicTransformer()
    
    return _business_transformer

-- ./app/etl/transformers/data_transformer.py --
"""
ðŸ”„ Data Transformer - COMPATIBILITY LAYER - CIRCULAR IMPORT FIXED
Maintains compatibility while redirecting to UnifiedTransformerRegistry

UPDATED: Removed circular import by defining get_transformer_registry locally
COMPATIBILITY: Existing code continues to work without changes
"""

# Import only what we need to avoid circular imports
from app.core.logging import LoggerMixin


# âœ… FIXED: Define get_transformer_registry locally (no circular import)
def get_transformer_registry():
    """Get transformer registry - now returns unified version"""
    from app.etl.transformers.unified_transformer import get_unified_transformer_registry
    return get_unified_transformer_registry()

# Legacy DataTransformer class for backwards compatibility
class DataTransformer:
    """
    DEPRECATED: Legacy DataTransformer class
    Use UnifiedTransformerRegistry instead for new code
    """
    
    def __init__(self):
        self._unified = get_unified_transformer_registry()
    
    def transform_dashboard_data(self, raw_data):
        return self._unified.transform_table_data('dashboard_data', raw_data)
    
    def transform_evolution_data(self, raw_data):
        return self._unified.transform_table_data('evolution_data', raw_data)
    
    def transform_assignment_data(self, raw_data):
        return self._unified.transform_table_data('assignment_data', raw_data)
    
    def transform_operation_data(self, raw_data):
        return self._unified.transform_table_data('operation_data', raw_data)
    
    def transform_productivity_data(self, raw_data):
        return self._unified.transform_table_data('productivity_data', raw_data)
    
    def get_transformation_stats(self):
        return self._unified.get_transformation_stats()
    
    def reset_stats(self):
        return self._unified.reset_all_stats()


# Legacy TransformerRegistry class for backwards compatibility
class TransformerRegistry:
    """
    DEPRECATED: Legacy TransformerRegistry class
    Use UnifiedTransformerRegistry instead for new code
    """
    
    def __init__(self):
        self._unified = get_unified_transformer_registry()
        self.transformer = DataTransformer()  # For legacy compatibility
    
    def transform_table_data(self, table_name, raw_data):
        return self._unified.transform_table_data(table_name, raw_data)
    
    def get_supported_tables(self):
        return self._unified.get_supported_tables()
    
    def get_transformation_stats(self):
        return self._unified.get_transformation_stats()


# Legacy transform_data function for backwards compatibility
def transform_data(table_name, raw_data):
    """Transform raw BigQuery data for a specific table"""
    registry = get_unified_transformer_registry()
    return registry.transform_table_data(table_name, raw_data)

-- ./app/etl/transformers/raw_data_transformer.py --
"""
ðŸ”„ Raw Data Transformers - BigQuery to PostgreSQL Raw Tables
FIXED: Column case sensitivity mapping for PostgreSQL compatibility
FIXED: Removed field duplications that caused INSERT column mismatches
FIXED: Inconsistencias en mÃ©todos helper y manejo de errores
ADDED: Debug logging for NULL primary key investigation
CRITICAL FIX: gestiones_unificadas CHECK constraint violations

ISSUE: Transform gestiones data was causing CHECK constraint violations:
- chk_gestiones_unificadas_contactabilidad: Invalid contactabilidad values
- chk_gestiones_unificadas_fecha_consistency: Date inconsistency

ROOT CAUSE: BigQuery view returns inconsistent values:
- 'No contacto' (354K records) âŒ Not in CHECK constraint 
- 'No Contacto' (182K records) âŒ Not in CHECK constraint
- 'Contacto No Efectivo' âœ… Valid
- 'Contacto Efectivo' âœ… Valid

SOLUTION:
âœ… Added contactabilidad value mapping to valid PostgreSQL values
âœ… Enforced fecha_gestion = DATE(timestamp_gestion) consistency  
âœ… Added data cleaning filters for invalid records
âœ… Proper fallback to valid default values
âœ… Enhanced logging for debugging problematic records
"""

from datetime import datetime, date, timezone
from typing import List, Dict, Any, Optional, Union
import re

from app.core.logging import LoggerMixin


class RawDataTransformer(LoggerMixin):
    """
    Raw data transformer for BigQuery â†’ PostgreSQL raw staging tables

    PRINCIPLE: Minimal transformation - just type conversion and basic cleaning
    PRESERVE: Original BigQuery data structure for business logic layer
    FIXED: Column name mapping for case sensitivity compatibility
    FIXED: Removed field duplications that caused INSERT errors
    FIXED: Inconsistencias en mÃ©todos helper y manejo de errores
    DEBUG: Added logging for NULL primary key investigation
    CRITICAL FIX: gestiones_unificadas CHECK constraint violations resolved
    """
    ISO_DATE_FORMAT = '%Y-%m-%d'

    def __init__(self):
        super().__init__()  # Fixed: removed extra parameter
        self.transformation_stats = {
            'records_processed': 0,
            'records_transformed': 0,
            'records_skipped': 0,
            'validation_errors': 0
        }

    def transform_raw_calendario(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Transform BigQuery calendario to PostgreSQL raw_calendario

        FIXED: Column names mapped to PostgreSQL lowercase convention
        FIXED: Removed field duplications that caused INSERT column mismatches
        DEBUG: Added logging for NULL primary key investigation
        """
        transformed_records = []

        # ðŸ” DEBUG: Log sample of raw data to investigate NULL primary keys
        if raw_data:
            self.logger.info(f"ðŸ” DEBUG: First raw record keys: {list(raw_data[0].keys())}")
            self.logger.info(f"ðŸ” DEBUG: First raw record sample: {dict(list(raw_data[0].items())[:5])}")

        for i, record in enumerate(raw_data):
            try:
                self.transformation_stats['records_processed'] += 1

                # ðŸ” DEBUG: Log primary key values for first few records
                raw_archivo = record.get('ARCHIVO')
                raw_periodo_date = record.get('periodo_date')

                if i < 3:  # Log first 3 records
                    self.logger.info(
                        f"ðŸ” DEBUG Record {i}: "
                        f"ARCHIVO='{raw_archivo}' (type: {type(raw_archivo)}), "
                        f"periodo_date='{raw_periodo_date}' (type: {type(raw_periodo_date)})"
                    )

                # Validate required primary key
                archivo = self._safe_string(record.get('ARCHIVO'))
                if not archivo:
                    self.logger.warning(f"âš ï¸ Skipping record {i}: archivo is null/empty (raw: '{raw_archivo}')")
                    self.transformation_stats['records_skipped'] += 1
                    continue

                # Validate periodo_date
                periodo_date = self._safe_date(record.get('periodo_date'))
                if not periodo_date:
                    self.logger.warning(f"âš ï¸ Skipping record {i}: periodo_date is null/empty (raw: '{raw_periodo_date}')")
                    self.transformation_stats['records_skipped'] += 1
                    continue

                # âœ… FIXED: Removed field duplications
                transformed = {
                    # Primary key - âœ… FIXED: Single definition only
                    'archivo': archivo,

                    # Campaign metadata - âœ… FIXED: Single definition only
                    'tipo_cartera': self._safe_string(record.get('TIPO_CARTERA')),

                    # Business dates - critical for campaign logic
                    'fecha_apertura': self._safe_date(record.get('fecha_apertura')),
                    'fecha_trandeuda': self._safe_date(record.get('fecha_trandeuda')),
                    'fecha_cierre': self._safe_date(record.get('fecha_cierre')),
                    'fecha_cierre_planificada': self._safe_date(record.get('FECHA_CIERRE_PLANIFICADA')),

                    # Campaign characteristics
                    'duracion_campana_dias_habiles': self._safe_int(record.get('DURACION_CAMPANA_DIAS_HABILES')),
                    'anno_asignacion': self._safe_int(record.get('ANNO_ASIGNACION')),
                    'periodo_asignacion': self._safe_string(record.get('PERIODO_ASIGNACION')),
                    'es_cartera_abierta': self._safe_bool(record.get('ES_CARTERA_ABIERTA')),
                    'rango_vencimiento': self._safe_string(record.get('RANGO_VENCIMIENTO')),
                    'estado_cartera': self._safe_string(record.get('ESTADO_CARTERA')),

                    # Time partitioning - âœ… FIXED: Use validated periodo_date
                    'periodo_mes': self._safe_string(record.get('periodo_mes')),
                    'periodo_date': periodo_date,  # Already validated above

                    # Campaign classification
                    'tipo_ciclo_campana': self._safe_string(record.get('tipo_ciclo_campana')),
                    'categoria_duracion': self._safe_string(record.get('categoria_duracion')),

                    # ETL metadata
                    'extraction_timestamp': self._safe_datetime(record.get('extraction_timestamp')) or datetime.now(timezone.utc)
                }

                # Validate required business date
                if not transformed['fecha_apertura']:
                    self.logger.warning(f"âš ï¸ Skipping record {i}: fecha_apertura is null/empty")
                    self.transformation_stats['records_skipped'] += 1
                    continue

                # ðŸ” DEBUG: Log successful transformation for first few records
                if i < 3:
                    self.logger.info(
                        f"âœ… DEBUG Record {i} transformed: "
                        f"archivo='{transformed['archivo']}', "
                        f"periodo_date='{transformed['periodo_date']}', "
                        f"fecha_apertura='{transformed['fecha_apertura']}'"
                    )

                transformed_records.append(transformed)
                self.transformation_stats['records_transformed'] += 1

            except Exception as e:
                self.logger.error(f"Error transforming raw_calendario record {i}: {str(e)}")
                self.transformation_stats['validation_errors'] += 1
                continue

        # ðŸ” DEBUG: Final transformation summary
        self.logger.info(
            f"ðŸ” DEBUG Summary: {len(raw_data)} raw â†’ {len(transformed_records)} transformed "
            f"(skipped: {self.transformation_stats['records_skipped']}, "
            f"errors: {self.transformation_stats['validation_errors']})"
        )

        return transformed_records

    def transform_raw_asignaciones(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Transform BigQuery asignaciones to PostgreSQL raw_asignaciones

        FIXED: Column names mapped to PostgreSQL lowercase convention
        """
        transformed_records = []

        for i, record in enumerate(raw_data):
            try:
                self.transformation_stats['records_processed'] += 1

                # Validate primary key components
                cod_luna = self._safe_string(record.get('cod_luna'))
                cuenta = self._safe_string(record.get('cuenta'))
                archivo = self._safe_string(record.get('archivo'))

                if not cod_luna or not cuenta or not archivo:
                    self.logger.warning(f"âš ï¸ Skipping asignaciones record {i}: missing primary key components")
                    self.transformation_stats['records_skipped'] += 1
                    continue

                transformed = {
                    # Primary key components
                    'cod_luna': cod_luna,
                    'cuenta': cuenta,
                    'archivo': archivo,

                    # Client information
                    'cliente': self._safe_string(record.get('cliente')),
                    'telefono': self._safe_string(record.get('telefono')),

                    # Business classification
                    'tramo_gestion': self._safe_string(record.get('tramo_gestion')),
                    'negocio': self._safe_string(record.get('negocio')),
                    'dias_sin_trafico': self._safe_string(record.get('dias_sin_trafico')),

                    # Risk scoring
                    'decil_contacto': self._safe_int(record.get('decil_contacto')),
                    'decil_pago': self._safe_int(record.get('decil_pago')),

                    # Account details
                    'min_vto': self._safe_date(record.get('min_vto')),
                    'zona': self._safe_string(record.get('zona')),
                    'rango_renta': self._safe_int(record.get('rango_renta')),
                    'campania_act': self._safe_string(record.get('campania_act')),

                    # Payment arrangement
                    'fraccionamiento': self._safe_string(record.get('fraccionamiento')),
                    'cuota_fracc_act': self._safe_string(record.get('cuota_fracc_act')),
                    'fecha_corte': self._safe_date(record.get('fecha_corte')),
                    'priorizado': self._safe_string(record.get('priorizado')),
                    'inscripcion': self._safe_string(record.get('inscripcion')),
                    'incrementa_velocidad': self._safe_string(record.get('incrementa_velocidad')),
                    'detalle_dscto_futuro': self._safe_string(record.get('detalle_dscto_futuro')),
                    'cargo_fijo': self._safe_string(record.get('cargo_fijo')),

                    # Client identification
                    'dni': self._safe_string(record.get('dni')),
                    'estado_pc': self._safe_string(record.get('estado_pc')),
                    'tipo_linea': self._safe_string(record.get('tipo_linea')),
                    'cod_sistema': self._safe_int(record.get('cod_sistema')),
                    'tipo_alta': self._safe_string(record.get('tipo_alta')),

                    # Technical metadata
                    'creado_el': self._safe_datetime(record.get('creado_el')),
                    'fecha_asignacion': self._safe_date(record.get('fecha_asignacion')),
                    'motivo_rechazo': self._safe_string(record.get('motivo_rechazo')),

                    # ETL metadata
                    'extraction_timestamp': self._safe_datetime(record.get('extraction_timestamp')) or datetime.now(timezone.utc)
                }

                transformed_records.append(transformed)
                self.transformation_stats['records_transformed'] += 1

            except Exception as e:
                self.logger.error(f"Error transforming raw_asignaciones record {i}: {str(e)}")
                self.transformation_stats['validation_errors'] += 1
                continue

        return transformed_records

    def transform_raw_trandeuda(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Transform BigQuery trandeuda to PostgreSQL raw_trandeuda

        MINIMAL: Type conversion + debt validation
        """
        transformed_records = []

        for i, record in enumerate(raw_data):
            try:
                self.transformation_stats['records_processed'] += 1

                # Validate primary key components
                cod_cuenta = self._safe_string(record.get('cod_cuenta'))
                nro_documento = self._safe_string(record.get('nro_documento'))
                archivo = self._safe_string(record.get('archivo'))
                monto_exigible = self._safe_decimal(record.get('monto_exigible'))

                if not cod_cuenta or not nro_documento or not archivo:
                    self.logger.warning(f"âš ï¸ Skipping trandeuda record {i}: missing primary key components")
                    self.transformation_stats['records_skipped'] += 1
                    continue

                # Skip zero debt records
                if monto_exigible is None or monto_exigible <= 0:
                    self.logger.warning(f"âš ï¸ Skipping trandeuda record {i}: invalid monto_exigible ({monto_exigible})")
                    self.transformation_stats['records_skipped'] += 1
                    continue

                transformed = {
                    # Primary key components
                    'cod_cuenta': cod_cuenta,
                    'nro_documento': nro_documento,
                    'archivo': archivo,

                    # Debt information
                    'fecha_vencimiento': self._safe_date(record.get('fecha_vencimiento')),
                    'monto_exigible': monto_exigible,

                    # Technical metadata
                    'creado_el': self._safe_datetime(record.get('creado_el')),
                    'fecha_proceso': self._safe_date(record.get('fecha_proceso')),
                    'motivo_rechazo': self._safe_string(record.get('motivo_rechazo')),

                    # ETL metadata
                    'extraction_timestamp': self._safe_datetime(record.get('extraction_timestamp')) or datetime.now(timezone.utc)
                }

                transformed_records.append(transformed)
                self.transformation_stats['records_transformed'] += 1

            except Exception as e:
                self.logger.error(f"Error transforming raw_trandeuda record {i}: {str(e)}")
                self.transformation_stats['validation_errors'] += 1
                continue

        return transformed_records

    def transform_raw_pagos(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Transform BigQuery pagos to PostgreSQL raw_pagos

        MINIMAL: Type conversion + payment validation
        """
        transformed_records = []

        for i, record in enumerate(raw_data):
            try:
                self.transformation_stats['records_processed'] += 1

                # Validate primary key components (for deduplication)
                nro_documento = self._safe_string(record.get('nro_documento'))
                fecha_pago = self._safe_date(record.get('fecha_pago'))
                monto_cancelado = self._safe_decimal(record.get('monto_cancelado'))

                if not nro_documento or not fecha_pago or monto_cancelado is None or monto_cancelado <= 0:
                    self.logger.warning(f"âš ï¸ Skipping pagos record {i}: invalid primary key or amount")
                    self.transformation_stats['records_skipped'] += 1
                    continue

                transformed = {
                    # Primary key components (deduplication key)
                    'nro_documento': nro_documento,
                    'fecha_pago': fecha_pago,
                    'monto_cancelado': monto_cancelado,

                    # System identification
                    'cod_sistema': self._safe_string(record.get('cod_sistema')),
                    'archivo': self._safe_string(record.get('archivo')),

                    # Technical metadata
                    'creado_el': self._safe_datetime(record.get('creado_el')),
                    'motivo_rechazo': self._safe_string(record.get('motivo_rechazo')),

                    # ETL metadata
                    'extraction_timestamp': self._safe_datetime(record.get('extraction_timestamp')) or datetime.now(timezone.utc)
                }

                transformed_records.append(transformed)
                self.transformation_stats['records_transformed'] += 1

            except Exception as e:
                self.logger.error(f"Error transforming raw_pagos record {i}: {str(e)}")
                self.transformation_stats['validation_errors'] += 1
                continue

        return transformed_records

    def transform_gestiones_unificadas(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ðŸ› ï¸ CRITICAL FIX: Transform BigQuery unified gestiones to PostgreSQL gestiones_unificadas
        
        FIXED: CHECK constraint violations by mapping inconsistent BigQuery values to valid PostgreSQL values
        
        ISSUE: BigQuery view returns:
        - 'No contacto' (354K records) âŒ Not in CHECK constraint 
        - 'No Contacto' (182K records) âŒ Not in CHECK constraint
        - 'Contacto No Efectivo' âœ… Valid
        - 'Contacto Efectivo' âœ… Valid
        
        SOLUTION: Map all variants to valid PostgreSQL CHECK constraint values
        """
        transformed_records = []

        for i, record in enumerate(raw_data):
            try:
                self.transformation_stats['records_processed'] += 1

                # Validate primary key components
                cod_luna = self._safe_string(record.get('cod_luna'))
                timestamp_gestion = self._safe_datetime(record.get('timestamp_gestion'))

                if not cod_luna or not timestamp_gestion:
                    self.logger.warning(f"âš ï¸ Skipping gestiones record {i}: missing primary key components")
                    self.transformation_stats['records_skipped'] += 1
                    continue

                # ðŸ› ï¸ CRITICAL FIX: Enforce date consistency for CHECK constraint
                fecha_gestion = timestamp_gestion.date()  # Always derive from timestamp to ensure consistency

                # ðŸ› ï¸ CRITICAL FIX: Map contactabilidad to valid PostgreSQL values
                raw_contactabilidad = self._safe_string(record.get('contactabilidad'))
                contactabilidad = self._map_contactabilidad_to_valid_value(raw_contactabilidad)

                # Skip records that couldn't be mapped to valid values
                if not contactabilidad:
                    self.logger.warning(f"âš ï¸ Skipping gestiones record {i}: unmappable contactabilidad '{raw_contactabilidad}'")
                    self.transformation_stats['records_skipped'] += 1
                    continue

                # ðŸ› ï¸ CRITICAL FIX: Validate canal_origen for CHECK constraint
                canal_origen = self._standardize_canal(record.get('canal_origen'))
                if canal_origen not in ['BOT', 'HUMANO']:
                    self.logger.warning(f"âš ï¸ Skipping gestiones record {i}: invalid canal_origen '{canal_origen}'")
                    self.transformation_stats['records_skipped'] += 1
                    continue

                transformed = {
                    # Primary key components
                    'cod_luna': cod_luna,
                    'timestamp_gestion': timestamp_gestion,
                    'fecha_gestion': fecha_gestion,  # âœ… FIXED: Always consistent with timestamp

                    # Channel information - âœ… FIXED: Validated for CHECK constraint
                    'canal_origen': canal_origen,

                    # Original management data
                    'management_original': self._safe_string(record.get('management_original')),
                    'sub_management_original': self._safe_string(record.get('sub_management_original')),
                    'compromiso_original': self._safe_string(record.get('compromiso_original')),

                    # Homologated classification - âœ… FIXED: Mapped to valid values
                    'nivel_1': self._safe_string(record.get('nivel_1')),
                    'nivel_2': self._safe_string(record.get('nivel_2')),
                    'contactabilidad': contactabilidad,  # âœ… FIXED: Guaranteed to be valid

                    # Business flags for KPI calculation - âœ… FIXED: Based on valid contactabilidad
                    'es_contacto_efectivo': (contactabilidad == 'Contacto Efectivo'),
                    'es_contacto_no_efectivo': (contactabilidad == 'Contacto No Efectivo'),
                    'es_compromiso': self._safe_bool(record.get('es_compromiso')),

                    # Weighting for business logic - FIXED: usar default_value en lugar de default
                    'peso_gestion': self._safe_int(record.get('peso_gestion'), default_value=1),

                    # ETL metadata
                    'extraction_timestamp': self._safe_datetime(record.get('extraction_timestamp')) or datetime.now(timezone.utc)
                }

                # ðŸ” DEBUG: Log mapping for first few records to verify fix
                if i < 5:
                    self.logger.info(
                        f"ðŸ› ï¸ DEBUG Record {i}: raw_contactabilidad='{raw_contactabilidad}' â†’ "
                        f"mapped='{contactabilidad}', canal='{canal_origen}', "
                        f"fecha_consistency: {fecha_gestion} == {timestamp_gestion.date()}"
                    )

                transformed_records.append(transformed)
                self.transformation_stats['records_transformed'] += 1

            except Exception as e:
                self.logger.error(f"Error transforming gestiones_unificadas record {i}: {str(e)}")
                self.transformation_stats['validation_errors'] += 1
                continue

        # ðŸ” DEBUG: Final transformation summary with fix metrics
        self.logger.info(
            f"ðŸ› ï¸ gestiones_unificadas FIXED: {len(raw_data)} raw â†’ {len(transformed_records)} transformed "
            f"(skipped: {self.transformation_stats['records_skipped']}, "
            f"errors: {self.transformation_stats['validation_errors']})"
        )

        return transformed_records

    def _map_contactabilidad_to_valid_value(self, raw_contactabilidad: Optional[str]) -> Optional[str]:
        """
        ðŸ› ï¸ CRITICAL FIX: Map inconsistent BigQuery contactabilidad values to valid PostgreSQL CHECK constraint values
        
        Maps:
        - 'No contacto' â†’ 'Contacto No Efectivo' 
        - 'No Contacto' â†’ 'Contacto No Efectivo'
        - 'Contacto No Efectivo' â†’ 'Contacto No Efectivo' (unchanged)
        - 'Contacto Efectivo' â†’ 'Contacto Efectivo' (unchanged)
        - Anything else â†’ 'SIN_CLASIFICAR'
        """
        if not raw_contactabilidad:
            return 'SIN_CLASIFICAR'
        
        # Normalize and map to valid PostgreSQL CHECK constraint values
        normalized = raw_contactabilidad.strip()
        
        # Direct matches (already valid)
        if normalized in ['Contacto Efectivo', 'Contacto No Efectivo', 'SIN_CLASIFICAR']:
            return normalized
        
        # Map inconsistent BigQuery values to valid PostgreSQL values
        if normalized.lower() in ['no contacto', 'no_contacto']:
            return 'Contacto No Efectivo'  # Map "no contact" variants to "ineffective contact"
        
        # Fallback for unmapped values
        self.logger.debug(f"ðŸ› ï¸ Unmapped contactabilidad value '{normalized}' â†’ using 'SIN_CLASIFICAR'")
        return 'SIN_CLASIFICAR'

    def transform_raw_homologacion_mibotair(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Transforms mibotair homologation data, casting 'peso' to integer."""
        transformed_records = []

        for i, record in enumerate(raw_data):
            try:
                self.transformation_stats['records_processed'] += 1

                # Convierte 'peso' a integer. Si falla o es nulo, pone 1.
                peso_str = record.get('peso')
                record['peso'] = int(peso_str) if peso_str and str(peso_str).isdigit() else 1

                transformed_records.append(record)
                self.transformation_stats['records_transformed'] += 1

            except (ValueError, TypeError) as e:
                self.logger.warning(f"âš ï¸ Error processing mibotair record {i}: {str(e)}, setting peso=1")
                record['peso'] = 1
                transformed_records.append(record)
                self.transformation_stats['records_transformed'] += 1

        return transformed_records

    def transform_raw_homologacion_voicebot(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Transforms voicebot homologation data, converting es_pdp to boolean."""
        transformed_records = []

        for i, record in enumerate(raw_data):
            try:
                self.transformation_stats['records_processed'] += 1

                # Convierte es_pdp_homologado (0 o 1) a un booleano
                record['es_pdp_homologado'] = bool(record.get('es_pdp_homologado', 0))

                transformed_records.append(record)
                self.transformation_stats['records_transformed'] += 1

            except Exception as e:
                self.logger.warning(f"âš ï¸ Error processing voicebot record {i}: {str(e)}")
                record['es_pdp_homologado'] = False
                transformed_records.append(record)
                self.transformation_stats['records_transformed'] += 1

        return transformed_records

    def transform_raw_ejecutivos(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Transforms agent data, handling null/empty documents."""
        transformed_records = []

        for i, record in enumerate(raw_data):
            try:
                self.transformation_stats['records_processed'] += 1

                # Si el documento es nulo o estÃ¡ vacÃ­o, lo reemplaza por "SIN DNI"
                if not record.get('document'):
                    record['document'] = 'SIN DNI'

                # Asegurarse de que el nombre sea un string
                record['nombre'] = str(record.get('nombre', '')).strip()

                transformed_records.append(record)
                self.transformation_stats['records_transformed'] += 1

            except Exception as e:
                self.logger.warning(f"âš ï¸ Error processing ejecutivos record {i}: {str(e)}")
                record['document'] = 'SIN DNI'
                record['nombre'] = ''
                transformed_records.append(record)
                self.transformation_stats['records_transformed'] += 1

        return transformed_records

    # =============================================================================
    # HELPER METHODS - Safe Type Conversion
    # =============================================================================

    @staticmethod
    def _safe_string(
            value: Union[str, Any],
            max_length: Optional[int] = None
    ) -> Optional[str]:
        """Safely converts input value to string with optional length validation.

        Args:
            value: Value to convert to string
            max_length: Optional maximum length limit for the resulting string

        Returns:
            Cleaned string value or None if conversion fails or result is empty

        Raises:
            ValueError: If max_length is negative
        """
        empty_values = (None, '')

        if value in empty_values:
            return None

        if max_length is not None and max_length < 0:
            raise ValueError("max_length must be a non-negative integer")

        try:
            result = str(value).strip()
            if not result:
                return None

            if max_length is not None:
                result = result[:max_length]

            return result

        except (ValueError, TypeError, AttributeError):
            return None

    @staticmethod
    def _safe_int(
            input_value: Union[str, int, float, None],
            default_value: Optional[int] = None
    ) -> Optional[int]:
        """Safely converts various input types to integer.

        Args:
            input_value: Value to convert (str, int, float, or None)
            default_value: Value to return if conversion fails (default: None)

        Returns:
            Converted integer value or default_value if conversion fails

        Examples:
            > _safe_int("123") == 123
            > _safe_int("12.3") == 12
            > _safe_int("12,300") == 12300
            > _safe_int(None) is None
        """
        # Early return for None or empty string
        if input_value is None or input_value == '':
            return default_value

        try:
            # Handle string input
            if isinstance(input_value, str):
                # Remove all non-numeric characters except minus sign
                cleaned_value = re.sub(r'[^\d\-]', '', input_value)
                return int(cleaned_value) if cleaned_value else default_value

            # Handle numeric input
            return int(float(input_value))

        except (ValueError, TypeError, OverflowError):
            return default_value

    @staticmethod
    def _safe_decimal(value: Any, default: Optional[float] = None) -> Optional[float]:
        """Safe decimal conversion - FIXED: Consistente con otros mÃ©todos helper

        Args:
            value: Value to convert to float
            default: Default value to return if conversion fails (default: None)

        Returns:
            Converted float value or default if conversion fails
        """
        if value is None or value == '':
            return default

        try:
            return float(value)
        except (ValueError, TypeError):
            return default

    @staticmethod
    def _safe_bool(value: Any) -> bool:
        """Safe boolean conversion"""
        if value is None:
            return False

        if isinstance(value, bool):
            return value

        if isinstance(value, str):
            return value.lower() in ('true', '1', 'yes', 'si', 'sÃ­')

        try:
            return bool(int(value))
        except (ValueError, TypeError):
            return False

    @staticmethod
    def _safe_date(value: Any) -> Optional[date]:
        """Safe date conversion with enhanced error handling"""
        if value is None:
            return None

        if isinstance(value, date):
            return value

        if isinstance(value, datetime):
            return value.date()

        if isinstance(value, str):
            return RawDataTransformer._parse_date_string(value)

        return None

    @staticmethod
    def _parse_date_string(date_string: str) -> Optional[date]:
        """Helper method to parse date strings in various formats.

        Supports:
        - ISO format (YYYY-MM-DD)
        - ISO datetime format with timezone
        - Common date formats
        """
        if not date_string or not date_string.strip():
            return None

        date_string = date_string.strip()

        try:
            # Try ISO format first (most common)
            return datetime.fromisoformat(date_string.replace('Z', '+00:00')).date()
        except ValueError:
            try:
                # Try common YYYY-MM-DD format
                return datetime.strptime(date_string, RawDataTransformer.ISO_DATE_FORMAT).date()
            except ValueError:
                # Could add more date formats here if needed
                return None

    @staticmethod
    def _safe_datetime(value: Any) -> Optional[datetime]:
        """Safe datetime conversion with consistent timezone handling"""
        if value is None:
            return None

        if isinstance(value, datetime):
            # Ensure timezone awareness - assume UTC if none specified
            if value.tzinfo is None:
                return value.replace(tzinfo=timezone.utc)
            return value

        if isinstance(value, str):
            if not value.strip():
                return None
            try:
                # Handle ISO format with or without timezone
                dt = datetime.fromisoformat(value.replace('Z', '+00:00'))
                # Ensure timezone awareness
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt
            except ValueError:
                return None

        return None

    def _standardize_canal(self, value: Any) -> str:
        """Standardize channel values with consistent defaults"""
        canal = self._safe_string(value, max_length=20)
        if not canal:
            return 'BOT'  # Default

        canal_upper = canal.upper()
        if canal_upper in ['BOT', 'VOICEBOT']:
            return 'BOT'
        elif canal_upper in ['HUMANO', 'HUMAN', 'CALL_CENTER', 'CALL CENTER']:
            return 'HUMANO'
        else:
            return 'BOT'  # Default fallback

    def get_transformation_stats(self) -> Dict[str, Any]:
        """Get transformation statistics"""
        return self.transformation_stats.copy()

    def reset_stats(self):
        """Reset transformation statistics"""
        self.transformation_stats = {
            'records_processed': 0,
            'records_transformed': 0,
            'records_skipped': 0,
            'validation_errors': 0
        }


# =============================================================================
# RAW TABLE TRANSFORMER REGISTRY
# =============================================================================

class RawTransformerRegistry:
    """Registry for raw table transformers"""

    def __init__(self):
        self.transformer = RawDataTransformer()

        # Map raw table names to transformation methods
        self.raw_transformer_mapping = {
            'raw_calendario': self.transformer.transform_raw_calendario,  # Fixed: removed lambda wrapper
            'raw_asignaciones': self.transformer.transform_raw_asignaciones,
            'raw_trandeuda': self.transformer.transform_raw_trandeuda,
            'raw_pagos': self.transformer.transform_raw_pagos,
            'gestiones_unificadas': self.transformer.transform_gestiones_unificadas,
            "raw_homologacion_mibotair": self.transformer.transform_raw_homologacion_mibotair,
            "raw_homologacion_voicebot": self.transformer.transform_raw_homologacion_voicebot,
            "raw_ejecutivos": self.transformer.transform_raw_ejecutivos,
        }

    def transform_raw_table_data(self, table_name: str, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Transform data for a specific raw table"""
        if table_name not in self.raw_transformer_mapping:
            raise ValueError(f"No raw transformer found for table: {table_name}")

        transformer_func = self.raw_transformer_mapping[table_name]
        return transformer_func(raw_data)

    def get_supported_raw_tables(self) -> List[str]:
        """Get list of supported raw table transformations  """
        return list(self.raw_transformer_mapping.keys())

    def get_transformation_stats(self) -> Dict[str, Any]:
        """Get transformation statistics"""
        return self.transformer.get_transformation_stats()


# ðŸŽ¯ Global raw transformer registry instance
_raw_transformer_registry: Optional[RawTransformerRegistry] = None

def get_raw_transformer_registry() -> RawTransformerRegistry:
    """Get singleton raw transformer registry instance"""
    global _raw_transformer_registry

    if _raw_transformer_registry is None:
        _raw_transformer_registry = RawTransformerRegistry()

    return _raw_transformer_registry
