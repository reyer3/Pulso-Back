"""
ğŸ¯ High-Performance PostgreSQL Loader with pure asyncpg
Efficient, asynchronous data loading for ETL pipelines.

Features:
- Pure asyncpg for maximum performance and low overhead
- Dynamic UPSERT statements with `ON CONFLICT DO UPDATE`
- Asynchronous streaming and batch processing
- Data validation and sanitization
- Detailed load statistics and error reporting

FIXED: Removed automatic fecha_procesamiento column that caused INSERT errors
"""

import time
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass
import logging

from app.database.connection import get_database_manager, DatabaseManager
from app.core.logging import LoggerMixin

logger = logging.getLogger(__name__)


@dataclass
class LoadResult:
    """Result information from a load operation"""
    table_name: str
    total_records: int
    inserted_records: int
    updated_records: int
    skipped_records: int
    load_duration_seconds: float
    status: str
    error_message: Optional[str] = None


class PostgresLoader(LoggerMixin):
    """
    A high-performance data loader for PostgreSQL using asyncpg.
    """

    def __init__(self, db_manager: Optional[DatabaseManager] = None):
        super().__init__()
        self.db_manager = db_manager
        self.max_batch_size = 1000
        self.connection_timeout = 30

    async def _get_db_manager(self) -> DatabaseManager:
        if self.db_manager is None:
            self.db_manager = await get_database_manager()
        return self.db_manager

    def _validate_and_sanitize_batch(
        self,
        data: List[Dict[str, Any]],
        primary_key: List[str]
    ) -> List[Dict[str, Any]]:
        """
        Validates and sanitizes a batch of data before loading.
        FIXED: Removed automatic fecha_procesamiento addition that caused errors
        """
        if not data:
            return []

        validated_data = []
        for i, record in enumerate(data):
            # Check for null primary key values
            if any(record.get(pk) is None for pk in primary_key):
                self.logger.warning(f"Record {i} has a null primary key value. Skipping.")
                continue

            # Sanitize and format data - FIXED: Only sanitize existing data
            sanitized_record = {}
            for key, value in record.items():
                if isinstance(value, str) and 'T' in value and ('Z'in value or '+' in value):
                    try:
                        sanitized_record[key] = datetime.fromisoformat(value.replace('Z', '+00:00'))
                    except ValueError:
                        sanitized_record[key] = value
                else:
                    sanitized_record[key] = value

            # REMOVED: Automatic fecha_procesamiento addition that caused INSERT errors
            # if 'fecha_procesamiento' not in sanitized_record:
            #     sanitized_record['fecha_procesamiento'] = datetime.now(timezone.utc)

            validated_data.append(sanitized_record)

        return validated_data

    async def load_data_batch(
        self,
        table_name: str,
        data: List[Dict[str, Any]],
        primary_key: List[str],
        upsert: bool = True,
        validate: bool = True
    ) -> LoadResult:
        """
        Loads a batch of data into a PostgreSQL table.
        IMPROVED: Better error handling and simpler INSERT construction
        """
        start_time = time.time()

        if not data:
            return LoadResult(
                table_name=table_name,
                total_records=0,
                inserted_records=0,
                updated_records=0,
                skipped_records=0,
                load_duration_seconds=0.0,
                status="success"
            )

        if validate:
            data = self._validate_and_sanitize_batch(data, primary_key)

        if not data:
            return LoadResult(
                table_name=table_name,
                total_records=0,
                inserted_records=0,
                updated_records=0,
                skipped_records=len(data),
                load_duration_seconds=time.time() - start_time,
                status="success",
                error_message="No valid records to load after validation."
            )

        db = await self._get_db_manager()
        pool = await db.get_pool()

        async with pool.acquire() as conn:
            try:
                # IMPROVED: Simpler INSERT construction for better debugging
                columns = list(data[0].keys())
                columns_str = ", ".join(f'"{c}"' for c in columns)
                placeholders = ", ".join(f"${i+1}" for i in range(len(columns)))
                pk_str = ", ".join(f'"{pk}"' for pk in primary_key)
                
                update_columns = [col for col in columns if col not in primary_key]
                update_str = ", ".join(f'"{col}" = EXCLUDED."{col}"' for col in update_columns)

                if upsert and update_columns:
                    query = f"""
                        INSERT INTO {table_name} ({columns_str})
                        VALUES ({placeholders})
                        ON CONFLICT ({pk_str}) DO UPDATE SET {update_str}
                    """
                else:
                    query = f"""
                        INSERT INTO {table_name} ({columns_str})
                        VALUES ({placeholders})
                    """

                # IMPROVED: Process records one by one for better error reporting
                inserted_count = 0
                for i, record in enumerate(data):
                    try:
                        values = [record[col] for col in columns]
                        await conn.execute(query, *values)
                        inserted_count += 1
                    except Exception as record_error:
                        self.logger.error(f"Failed to insert record {i} into {table_name}: {record_error}")
                        self.logger.debug(f"Failed record data: {record}")
                        # Continue with other records instead of failing the entire batch
                        continue
                
                duration = time.time() - start_time
                self.logger.info(f"Loaded {inserted_count}/{len(data)} records into {table_name} in {duration:.2f}s.")

                return LoadResult(
                    table_name=table_name,
                    total_records=len(data),
                    inserted_records=inserted_count,
                    updated_records=0,  # Simplified for performance
                    skipped_records=len(data) - inserted_count,
                    load_duration_seconds=duration,
                    status="success" if inserted_count > 0 else "failed"
                )

            except Exception as e:
                duration = time.time() - start_time
                error_msg = f"Failed to load data into {table_name}: {e}"
                self.logger.error(error_msg)
                return LoadResult(
                    table_name=table_name,
                    total_records=len(data),
                    inserted_records=0,
                    updated_records=0,
                    skipped_records=len(data),
                    load_duration_seconds=duration,
                    status="failed",
                    error_message=error_msg
                )

    async def load_data_streaming(
        self,
        table_name: str,
        data_stream: AsyncGenerator[List[Dict[str, Any]], None],
        primary_key: List[str],
        upsert: bool = True
    ) -> LoadResult:
        """
        Loads data from an async stream in batches.
        """
        start_time = time.time()
        total_records, total_inserted, total_skipped = 0, 0, 0
        errors = []

        async for batch in data_stream:
            if not batch:
                continue

            batch_result = await self.load_data_batch(
                table_name,
                batch,
                primary_key,
                upsert=upsert
            )

            total_records += batch_result.total_records
            total_inserted += batch_result.inserted_records
            total_skipped += batch_result.skipped_records

            if batch_result.status == "failed":
                errors.append(batch_result.error_message)

        duration = time.time() - start_time
        status = "success" if not errors else "partial_success"
        error_message = "; ".join(errors) if errors else None

        return LoadResult(
            table_name=table_name,
            total_records=total_records,
            inserted_records=total_inserted,
            updated_records=0,
            skipped_records=total_skipped,
            load_duration_seconds=duration,
            status=status,
            error_message=error_message
        )

    async def truncate_and_load(
        self,
        table_name: str,
        data: List[Dict[str, Any]],
        primary_key: List[str]
    ) -> LoadResult:
        """
        Truncates a table and then loads new data.
        """
        start_time = time.time()
        db = await self._get_db_manager()

        try:
            await db.execute_query(f"TRUNCATE TABLE {table_name} RESTART IDENTITY")
            self.logger.info(f"Truncated table: {table_name}")

            load_result = await self.load_data_batch(
                table_name,
                data,
                primary_key,
                upsert=False
            )

            load_result.load_duration_seconds = time.time() - start_time
            return load_result

        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"Failed truncate and load for {table_name}: {e}"
            self.logger.error(error_msg)
            return LoadResult(
                table_name=table_name,
                total_records=len(data),
                inserted_records=0,
                updated_records=0,
                skipped_records=len(data),
                load_duration_seconds=duration,
                status="failed",
                error_message=error_msg
            )

async def get_loader() -> PostgresLoader:
    """Returns a configured instance of the PostgresLoader."""
    db_manager = await get_database_manager()
    return PostgresLoader(db_manager)
